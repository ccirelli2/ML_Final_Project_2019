lr.m1.cv.loocv = train(num ~ age + sex + cp + trestbps + chol + fbs + restecg + thalach + exang + oldpeak + slope + ca + thal, # model to fit
data = data.cat,                                                       # data
method = 'glm',                                                        # general linear model
family = 'binomial',                                                   # binomial for logistic regression
trControl = data_ctrl_loocv                                                  # pass training control parameters.
)
lr.m1.cv.loocv
lda.m1.cv =        train(num ~., # model to fit
data = data.cat,                                                       # data
method = 'lda',                                                        # general linear model
family = 'binomial',                                                   # binomial for logistic regression
trControl = data_ctrl_loocv                                                  # pass training control parameters.
)
lda.m1.cv.loocv =        train(num ~., # model to fit
data = data.cat,                                                       # data
method = 'lda',                                                        # general linear model
family = 'binomial',                                                   # binomial for logistic regression
trControl = data_ctrl_loocv                                                  # pass training control parameters.
)
lda.m1.cv.loocv
qda.m1.cv,loocv =        train(num ~., # model to fit
data = data.cat,                                                       # data
method = 'qda',                                                        # general linear model
family = 'binomial',                                                   # binomial for logistic regression
trControl = data_ctrl_loocv                                                  # pass training control parameters.
)
qda.m1.cv.loocv =        train(num ~., # model to fit
data = data.cat,                                                       # data
method = 'qda',                                                        # general linear model
family = 'binomial',                                                   # binomial for logistic regression
trControl = data_ctrl_loocv                                                  # pass training control parameters.
)
qda.m1.cv.loocv
# Clean Namespace
rm(list = ls())
# LOAD LIBRARIES
library(RMySQL)
# CLEAR NAMESPACE
rm(list = ls())
# LOAD LIBRARIES
library(RMySQL)
# SETUP CONNECTION TO DB
mydb <- dbConnect(RMySQL::MySQL(), user='ccirelli2',
password='Work4starr', dbname='GSU',
host = "127.0.0.1")
# Get List of Tables
dbListTables(mydb)
query1_alldata = dbSendQuery(mydb, '
SELECT
MONTH(pickup_datetime) AS "MONTH",
DAY(pickup_datetime) AS "DAY",
pickup_x,
pickup_y,
dropoff_x,
dropoff_y,
duration
FROM ML_FinProj_GothamCab_Train
WHERE duration != 0
LIMIT 1000')
result_q1 = fetch(query1_alldata, n = -1)
# Query 2:  Average Duration By Month
query2_rel_month_duration = dbSendQuery(mydb, '
SELECT
MONTH(pickup_datetime) AS "MONTH",
ROUND(AVG(duration),0) AS "AVERAGE_DURATION"
FROM GSU.ML_FinProj_GothamCab_Train
GROUP BY MONTH(pickup_datetime)
ORDER BY ROUND(AVG(duration),0);')
result_q2 = fetch(query2_rel_month_duration, n = -1)
barplot(data_q2$AVERAGE_DURATION, names.arg = data_q2$MONTH,
main = "Avg Duration By Month",
xlab = "Month",
ylab = "Duration")
barplot(data_q2$AVERAGE_DURATION, names.arg = result_q2$MONTH,
main = "Avg Duration By Month",
xlab = "Month",
ylab = "Duration")
barplot(result_q2$AVERAGE_DURATION, names.arg = result_q2$MONTH,
main = "Avg Duration By Month",
xlab = "Month",
ylab = "Duration")
# Query 3 - Relationship of Average Duration By Day of Week
query3_rel_day_duration = dbSendQuery(mydb, '
SELECT
DAY(pickup_datetime) AS "DAY",
ROUND(AVG(duration),0) AS "AVERAGE_DURATION"
FROM GSU.ML_FinProj_GothamCab_Train
GROUP BY DAY(pickup_datetime)
ORDER BY DAY(pickup_datetime);')
result_q3 = fetch(query3_rel_day_duration, n = -1)
barplot(result_q3$AVERAGE_DURATION,
names.arg = result_q3$DAY,
main = "Average Duration By Day Of Week",
xlab = "Day",
ylab = "Duration")
test_date = 2012-02-01
test_date = weekdays(as.Date(2012-02-01))
test_date = weekdays(as.Date('2012-02-01'))
test_date
test_date = weekdays(as.Date('2034-01-01'))
test_date
query4_rel_weekday_duration = dbSendQuery(mydb, '
SELECT
Weekday,
ROUND(AVG(duration),0) AS "AVERAGE_DURATION"
FROM GSU.ML_FinProj_GothamCab_Train
GROUP BY Weekday
ORDER BY DAY(pickup_datetime);')
query4_rel_weekday_duration = dbSendQuery(mydb, '
SELECT
Weekday,
ROUND(AVG(duration),0) AS "AVERAGE_DURATION"
FROM GSU.ML_FinProj_GothamCab_Train
GROUP BY Weekday
ORDER BY Weekday;')
result_q4 = fetch(query4_rel_weekday_duration, n = -1)
barplot(result_q3$AVERAGE_DURATION,
names.arg = result_q3$DAY,
main = "Average Duration By Weekday",
xlab = "Weekday",
ylab = "Duration")
barplot(result_q4$AVERAGE_DURATION,
names.arg = result_q4$Weekday,
main = "Average Duration By Weekday",
xlab = "Weekday",
ylab = "Duration")
print(result_q4.head())
print(result_q4)
boxplot(result_q4)
boxplot(result_q1$duration)
result_q1$duration
plot(result_q1$duration)
boxplot(result_q1$duration)
histogram(result_q1$duration)
hist(result_q1$duration)
density(result_q1$duration)
d = density(result_q1$duration)
plot(d)
query5_rel_route_duration = dbSendQuery(mydb, '
SELECT
pickup_x,
pickup_y,
dropoff_x,
dropoff_y,
COUNT(duration) AS "Route Count"
FROM GSU.ML_FinProj_GothamCab_Train
GROUP BY pickup_x, pickup_y, dropoff_x, dropoff_y
ORDER BY COUNT(duration) DESC'
)
# Plot Durations
plot(result_q1$duration)
boxplot(result_q1$duration)
hist(result_q1$duration)
d = density(result_q1$duration)
plot(d)
barplot(model.rse,  names.arg = model.name)
## CLEAR NAMESPACE________________________________________________________________________
rm(list = ls())
## IMPORT LIBRARIES_______________________________________________________________________
library(lattice)
library(ggplot2)
library(caret)  # used for parameter tuning
library(glmnet)
library(pls)
library(ISLR)
## CREATE DATASET_________________________________________________________________________
setwd('/home/ccirelli2/Desktop/Repositories/ML_Final_Project_2019/Gotham_Cabs/data')
s1.50k.nolimits        = read.csv('sample1_50k.csv')[2:12]                          #[2:12] drop datetime col.
s2.100k.nolimits       = read.csv('sample1_100k.csv')[2:12]
s3.250k.nolimits       = read.csv('sample1_250k.csv')[2:12]
s4.50k.wlimits         = read.csv('sample2_wlimits_50k.csv')[2:12]
s5.100k.wlimits        = read.csv('sample2_wlimits_100k.csv')[2:12]
s6.250k.wlimits        = read.csv('sample2_wlimits_250k.csv')[2:12]
# SET SEED FOR ENTIRE CODE________________________________________________________________
set.seed(123)
# RANDOMIZE DATA__________________________________________________________________________
s1.50k.nolimits_ran    = s1.50k.nolimits[sample(nrow(s1.50k.nolimits)),]
s2.100k.nolimits_ran   = s2.100k.nolimits[sample(nrow(s2.100k.nolimits)),]
s3.250k.nolimits_ran   = s3.250k.nolimits[sample(nrow(s3.250k.nolimits)),]
s4.50k.wlimits_ran     = s4.50k.wlimits[sample(nrow(s4.50k.wlimits)), ]
s5.100k.wlimits_ran    = s5.100k.wlimits[sample(nrow(s5.100k.wlimits)), ]
s6.250k.wlimits_ran    = s6.250k.wlimits[sample(nrow(s6.250k.wlimits)), ]
# Calculate Number of Training Observations
train_nrows_50k  = (nrow(s1.50k.nolimits)  * .7)
train_nrows_100k = (nrow(s2.100k.nolimits)   * .7)
train_nrows_250k = (nrow(s3.250k.nolimits)   * .7)
# Train
s1.train = s1.50k.nolimits_ran[1:  (nrow(s1.50k.nolimits_ran)  * .7), ]
s2.train = s2.100k.nolimits_ran[1: (nrow(s2.100k.nolimits_ran) * .7), ]
s3.train = s3.250k.nolimits_ran[1: (nrow(s2.100k.nolimits_ran) * .7), ]
s4.train = s4.50k.wlimits_ran[1:   (nrow(s4.50k.wlimits_ran)  * .7), ]
s5.train = s5.100k.wlimits_ran[1:  (nrow(s5.100k.wlimits_ran)  * .7), ]
s6.train = s6.250k.wlimits_ran[1:  (nrow(s6.250k.wlimits_ran)  * .7), ]
# Test
s1.test = s1.50k.nolimits_ran[train_nrows_50k:    nrow(s1.50k.nolimits_ran), ] # Index from training to total
s2.test = s2.100k.nolimits_ran[train_nrows_100k:  nrow(s2.100k.nolimits_ran), ]
s3.test = s3.250k.nolimits_ran[train_nrows_250k:  nrow(s3.250k.nolimits_ran), ]
s4.test = s4.50k.wlimits_ran[train_nrows_50k:     nrow(s4.50k.wlimits_ran), ]
s5.test = s5.100k.wlimits_ran[train_nrows_100k:   nrow(s5.100k.wlimits_ran), ]
s6.test = s6.250k.wlimits_ran[train_nrows_250k:   nrow(s6.250k.wlimits_ran), ]
# M1:   TRAIN MULTILINEAR MODEL___________________________________________________
m1.mlr = lm(duration ~ ., data = s1.train)
m1.summary = summary(m1.mlr)
m1.rse = sqrt((sum(m1.summary$residuals^2)) / nrow(s1.train))
m1.summary
m1.sqrt.sq.coeff = m1.summary$coefficients
m1.sqrt.sq.coeff
m1.sqrt.sq.coeff = m1.summary$coefficients[:,1]
m1.sqrt.sq.coeff = m1.summary$coefficients[,1]
m1.sqrt.sq.coeff
sqrt(m1.sqrt.sq.coeff^2)
sum(sqrt(m1.sqrt.sq.coeff^2))
m1.sqrt.sq.coeff = sum(sqrt(m1.sqrt.sq.coeff^2))
m1.sqrt.sq.coeff
m1.sqrt.sq.coeff = m1.summary$coefficients[,1]
m1.sqrt.sq.coeff
m1.sqrt.sq.coeff = sum(sqrt(m1.summary$coefficients[,1]^2))                   # square coefficients, take square root and then sum.
m1.sqrt.sq.coeff
m_cv
# Separate Target & Feature Values
s1_y = s1.train$duration
s1_x = as.matrix(s1.train[,2:11])
s2_y = s4.train$duration
s2_x = as.matrix(s4.train[,2:11])
s3_y = s2.train$duration
s3_x = as.matrix(s2.train[,2:11])
s4_y = s5.train$duration
s4_x = as.matrix(s5.train[,2:11])
# Generate Grid Possible Values Lambda
grid = 10^seq(from = 10, to = -2, length = 100)                 #length = desired length of sequence
# Train MLR Using All Lambdas Grid
m_cv <- glmnet(s3_x, s3_y, alpha = 0, lambda = grid, standardize = TRUE)
m_cv
# Train MLR Using All Lambdas Grid
m_cv <- glmnet(s1_x, s1_y, alpha = 0, lambda = grid, standardize = TRUE)
m_cv
m_cv$df
?glmnet
summary(m_cv)
# 25th Lambda
print(paste('25th Lambda =>', m_cv$lambda[25]))           # Value of 25th Lambda
print((coef(m_cv)[,25]))         # Coefficients derived from 25th Lambda
# Train MLR Using All Lambdas Grid
m2.ridge <- glmnet(s1_x, s1_y, alpha = 0, lambda = grid, standardize = TRUE)
# 25th Lambda
print(paste('25th Lambda =>', m2.ridge$lambda[25]))           # Value of 25th Lambda
print((coef(m2.ridge)[,25]))         # Coefficients derived from 25th Lambda
print((coef(m2.ridge)[,25]))         # Coefficients derived from 25th Lambda
m2.coeff_25th = coef(m2.ridge)[,25]
m2.sqrt.sq.coeff = sum(sqrt(m2.coeff_25th^2))
m2.sqrt.sq.coeff
# 75th Lambda
print(paste('75th Lambda =>', m_cv$lambda[75]))           # Value of 75th Lambda
print((coef(m_cv)[,75]))         # Coefficients derived from 75th Lambda
m2.coeff_75th = coef(m2.ridge)[,75]
m2.sqrt.sq.coeff = sum(sqrt(m2.coeff_75th^2))
m2.sqrt.sq.coeff.25th = sum(sqrt(m2.coeff_25th^2))
m2.sqrt.sq.coeff
m2.sqrt.sq.coeff.75th = sum(sqrt(m2.coeff_75th^2))
m2.sqrt.sq.coeff.75th
m2.coeff_75th = coef(m2.ridge)[,75]
# 25th Lambda
print(paste('25th Lambda =>', m2.ridge$lambda[25]))           # Value of 25th Lambda
print((coef(m2.ridge)[,25]))         # Coefficients derived from 25th Lambda
m2.coeff_25th = coef(m2.ridge)[,25]
m2.sqrt.sq.coeff.25th = sum(sqrt(m2.coeff_25th^2))
m2.sqrt.sq.coeff
m2.sqrt.sq.coeff.75th
# 75th Lambda
print(paste('75th Lambda =>', m_cv$lambda[75]))           # Value of 75th Lambda
print((coef(m_cv)[,75]))         # Coefficients derived from 75th Lambda
m2.coeff_75th = coef(m2.ridge)[,75]
m2.sqrt.sq.coeff.75th = sum(sqrt(m2.coeff_75th^2))
m2.sqrt.sq.coeff.75th
# 75th Lambda
print(paste('75th Lambda =>', m_cv$lambda[50]))           # Value of 75th Lambda
print((coef(m_cv)[,75]))         # Coefficients derived from 75th Lambda
m2.coeff_75th = coef(m2.ridge)[,50]
m2.sqrt.sq.coeff.75th = sum(sqrt(m2.coeff_75th^2))
m2.sqrt.sq.coeff.75th
barplot(model.rse,  names.arg = model.name)
# Ridge
ridge_model = ridge_cv(s4_x, s4_y, grid, c_alpha = 0, opt_lambda = TRUE, c_plot = FALSE)
ridge_cv <- function(X, Y, grid, c_alpha, opt_lambda, c_plot){
# Train Cross Validation Model
m_cv <- cv.glmnet(X, Y, alpha = c_alpha, lambda = grid, standardize = TRUE, nfolds = 10)
# Plot RSE vs Lambda Selection
if(c_plot == TRUE){
plot(m_cv, main = "MLR - 10KFOLD USING RIDGE")
}
# Get Best Lambda
cv_lambda = m_cv$lambda.min
if(opt_lambda == TRUE){
print(paste('Optimal lambda =>', round(cv_lambda, 2)))
}
# Fit Model w/ Best Lambda
m_optimal <- glmnet(X, Y, alpha = c_alpha, lambda = cv_lambda, standardize = TRUE)
y_hat_cv <- predict(m_optimal, X)
model_cv_rse = sqrt(sum((Y - y_hat_cv)^2) / (length(Y) - 2))
return(model_cv_rse)
}
# Ridge
ridge_model = ridge_cv(s4_x, s4_y, grid, c_alpha = 0, opt_lambda = TRUE, c_plot = FALSE)
ridge_model
# Lasso
lasso_model = ridge_cv(s4_x, s4_y, grid, c_alpha = 1, opt_lambda = TRUE, c_plot = FALSE)
lasso_model
barplot(model.rse,  names.arg = model.name)
## Results_______________________________________________________________________________
model.name = list('m1.mlr.s1', 'm2.ridge.s1.50k.nl', 'm3.ridge.s2.50k.wl', 'm4.ridge.s4.100k.wl', 'm5.lasso.s1.50k.nl',
'm6.lasso.s2.50k.wl', 'm7.lasso.s4.100k.wl')
model.rse = c(280.2, 280.1613, 246.52, 241.053, 280.1571, 246.52, 241.053)
barplot(model.rse,  names.arg = model.name)
# Train MLR Using All Lambdas Grid
m3.ridge <- glmnet(s1_x, s1_y, alpha = 1, lambda = grid, standardize = TRUE)
# 25th Lambda
print(paste('25th Lambda =>', m3.ridge$lambda[25]))           # Value of 25th Lambda
print((coef(m3.ridge)[,25]))         # Coefficients derived from 25th Lambda
# Train MLR Using All Lambdas Grid
m3.lasso <- glmnet(s1_x, s1_y, alpha = 1, lambda = grid, standardize = TRUE)
# 25th Lambda
print(paste('25th Lambda =>', m3.lasso$lambda[25]))           # Value of 25th Lambda
print((coef(m3.lasso)[,25]))         # Coefficients derived from 25th Lambda
m2.coeff_25th = coef(m2.ridge)[,25]
m2.sqrt.sq.coeff.25th = sum(sqrt(m2.coeff_25th^2))
m2.sqrt.sq.coeff
m2.coeff_25th = coef(m2.lasso)[,25]
m2.coeff_25th = coef(m3.lasso)[,25]
m2.sqrt.sq.coeff.25th = sum(sqrt(m3.coeff_25th^2))
m3.coeff_25th = coef(m3.lasso)[,25]
m3.sqrt.sq.coeff.25th = sum(sqrt(m3.coeff_25th^2))
m3.sqrt.sq.coeff
m3.sqrt.sq.coeff.25th
# 25th Lambda
print(paste('25th Lambda =>', m3.lasso$lambda[25]))           # Value of 25th Lambda
grid
print((coef(m3.lasso)[,25]))                                  # Odd, all coefficeints are zero
print((coef(m3.lasso)[,50]))                                  # Odd, all coefficeints are zero
print((coef(m3.lasso)[,75]))                                  # Odd, all coefficeints are zero
print((coef(m3.lasso)[,25]))                                  # Odd, all coefficeints are zero
m3.coeff_25th = coef(m3.lasso)[,25]
m3.sqrt.sq.coeff.25th = sum(sqrt(m3.coeff_25th^2))
m3.sqrt.sq.coeff.25th
# 75th Lambda
print(paste('50th Lambda =>', m_cv$lambda[50]))           # Value of 75th Lambda
print((coef(m_cv)[,50]))         # Coefficients derived from 75th Lambda
m2.coeff_50th = coef(m2.ridge)[,50]
m3.coeff_50th = coef(m3.lasso)[,50]
m3.sqrt.sq.coeff.50th = sum(sqrt(m3.coeff_50th^2))
m3.coeff_50th
m3.sqrt.sq.coeff.50th
m3.sqrt.sq.coeff.25th
print((coef(m3.coeff_25th)[,50]))         # Coefficients derived from 75th Lambda
print((coef(m3.coeff_25th)[,50]))         # Coefficients derived from 75th Lambda
print((coef(m3.lasso)[,50]))         # Coefficients derived from 75th Lambda
m3.coeff_50th = coef(m3.lasso)[,50]
m3.sqrt.sq.coeff.50th = sum(sqrt(m3.coeff_50th^2))
m3.sqrt.sq.coeff.50th
# Train MLR Using All Lambdas Grid
m3.lasso <- glmnet(s1_x, s1_y, alpha = 1, lambda = grid, standardize = TRUE)
print((coef(m3.lasso)[,50]))         # Coefficients derived from 75th Lambda
m3.coeff_50th = coef(m3.lasso)[,50]
m3.coeff_25th^2
m3.coeff_50th^2
sum(m3.coeff_50th^2)
list(m3.coeff_50th^2)
barplot(model.rse,  names.arg = model.name, cex.names = 1.5)
barplot(model.rse,  names.arg = model.name, cex.names = .5)
barplot(model.rse,  names.arg = model.name, cex.names = .75)
barplot(model.rse,  names.arg = model.name, cex.names = .5
barplot(model.rse,  names.arg = model.name, cex.names = .5)
barplot(model.rse,  names.arg = model.name, cex.names = .5)
barplot(model.rse,  names.arg = model.name, cex.names = .5, cex = .8)
barplot(model.rse,  names.arg = model.name, cex.names = .5, rot = 50)
barplot(model.rse,  names.arg = model.name, cex.names = .5, las = 2)
barplot(model.rse,  names.arg = model.name, cex.names = .75, las = 2)
barplot(model.rse,  names.arg = model.name, cex.names = .75, las = 2,
main = "Comparison Ridge & Lasso RSE")
barplot(model.rse,  names.arg = model.name, cex.names = .75, las = 2,
main = "Comparison Ridge & Lasso RSE",
xlab = 'RSE')
barplot(model.rse,  names.arg = model.name, cex.names = .75, las = 2,
main = "Comparison Ridge & Lasso RSE",
ylab = 'RSE')
# Ridge
ridge_model = ridge_cv(s4_x, s4_y, grid, c_alpha = 0, opt_lambda = TRUE, c_plot = FALSE)
ridge_model
# Lasso
lasso_model = ridge_cv(s4_x, s4_y, grid, c_alpha = 1, opt_lambda = TRUE, c_plot = FALSE)
lasso_model
## CLEAR NAMESPACE________________________________________________________________________
rm(list = ls())
## IMPORT LIBRARIES_______________________________________________________________________
library(lattice)
library(ggplot2)
library(caret)  # used for parameter tuning
library(glmnet)
library(pls)
library(ISLR)
## CREATE DATASET_________________________________________________________________________
setwd('/home/ccirelli2/Desktop/Repositories/ML_Final_Project_2019/Gotham_Cabs/data')
s1.50k.nolimits        = read.csv('sample1_50k.csv')[2:12]                          #[2:12] drop datetime col.
s2.100k.nolimits       = read.csv('sample1_100k.csv')[2:12]
s3.250k.nolimits       = read.csv('sample1_250k.csv')[2:12]
s4.50k.wlimits         = read.csv('sample2_wlimits_50k.csv')[2:12]
s5.100k.wlimits        = read.csv('sample2_wlimits_100k.csv')[2:12]
s6.250k.wlimits        = read.csv('sample2_wlimits_250k.csv')[2:12]
# SET SEED FOR ENTIRE CODE________________________________________________________________
set.seed(123)
# RANDOMIZE DATA__________________________________________________________________________
s1.50k.nolimits_ran    = s1.50k.nolimits[sample(nrow(s1.50k.nolimits)),]
s2.100k.nolimits_ran   = s2.100k.nolimits[sample(nrow(s2.100k.nolimits)),]
s3.250k.nolimits_ran   = s3.250k.nolimits[sample(nrow(s3.250k.nolimits)),]
s4.50k.wlimits_ran     = s4.50k.wlimits[sample(nrow(s4.50k.wlimits)), ]
s5.100k.wlimits_ran    = s5.100k.wlimits[sample(nrow(s5.100k.wlimits)), ]
s6.250k.wlimits_ran    = s6.250k.wlimits[sample(nrow(s6.250k.wlimits)), ]
# Create Training Method - CV & 10kfold
train.control = trainControl(method = 'cv', number = 10)
# Train Model
m1.backward = train(duration ~ ., data = s1.50k.nolimits_ran,
method = 'leapBackward',                                 # Step selection
tuneGrid = data.frame(nvmax = 1:11),                     # Number of features to consider in the model
trControl = train.control)                               # Cross Validation technique
m1.backward$results
?train
model_opt < function(dataset, opt_method, num_param, cv_grid, result2return){
'opt_method:        either leapBackward or leapForward
results:           A dataframe with the training error rate and values for the tuning params
bestTune           A dataframe with the final parameters
finalModel         Aa fit object using the best parameters'
# Train Model
m0 = train(duration ~ ., data = dataset,
method = opt_method,                                     # Step selection
tuneGrid = data.frame(nvmax = 1:11),                     # Number of features to consider in the model
trControl = train.control)                               # Cross Validation technique
# Results
if (result2return == 'results'){
print(paste('Results:',m1.backward$results))
return(m1.backward$results)}
# Final Model - # An asterisk specifies that the feature was included in the model
else if (result2return == 'finalModel'){
print(paste('Final Model:',summary(m1.backward$finalModel)))
return(summary(m1.backward$finalModel))}
# END
}
model_opt < function(dataset, opt_method, num_param, cv_grid, result2return){
'opt_method:        either leapBackward or leapForward
results:           A dataframe with the training error rate and values for the tuning params
bestTune           A dataframe with the final parameters
finalModel         Aa fit object using the best parameters'
# Train Model
m0 = train(duration ~ ., data = dataset,
method = opt_method,                                     # Step selection
tuneGrid = data.frame(nvmax = 1:11),                     # Number of features to consider in the model
trControl = train.control)                               # Cross Validation technique
# Results
if (result2return == 'results'){
print(paste('Results:',m1.backward$results))
return(m1.backward$results)}
# Final Model - # An asterisk specifies that the feature was included in the model
else if (result2return == 'finalModel'){
print(paste('Final Model:',summary(m1.backward$finalModel)))
return(summary(m1.backward$finalModel))}
# END
}
model_opt <- function(dataset, opt_method, num_param, cv_grid, result2return){
'opt_method:        either leapBackward or leapForward
results:           A dataframe with the training error rate and values for the tuning params
bestTune           A dataframe with the final parameters
finalModel         Aa fit object using the best parameters'
# Train Model
m0 = train(duration ~ ., data = dataset,
method = opt_method,                                     # Step selection
tuneGrid = data.frame(nvmax = 1:11),                     # Number of features to consider in the model
trControl = train.control)                               # Cross Validation technique
# Results
if (result2return == 'results'){
print(paste('Results:',m1.backward$results))
return(m1.backward$results)}
# Final Model - # An asterisk specifies that the feature was included in the model
else if (result2return == 'finalModel'){
print(paste('Final Model:',summary(m1.backward$finalModel)))
return(summary(m1.backward$finalModel))}
# END
}
model_opt(s1.50k.nolimits_ran, 'leapBackward', 11, train.control, 'results')
model_opt(s1.50k.nolimits_ran, 'leapBackward', 11, train.control, 'finalModel')
model_opt <- function(dataset, opt_method, num_param, cv_grid, result2return){
'opt_method:        either leapBackward or leapForward
results:           A dataframe with the training error rate and values for the tuning params
bestTune           A dataframe with the final parameters
finalModel         Aa fit object using the best parameters'
# Train Model
m0 = train(duration ~ ., data = dataset,
method    = opt_method,                                   # Step selection
tuneGrid  = data.frame(nvmax = 1 : num_param),            # Number of features to consider in the model
trControl = cv_grid)                                      # Cross Validation technique
# Results
if (result2return == 'results'){
print(paste('Results:',m0$results))
return(m0$results)}
# Final Model - # An asterisk specifies that the feature was included in the model
else if (result2return == 'finalModel'){
print(paste('Final Model:',summary(m0$finalModel)))
return(summary(m1.backward$finalModel))}
else if (result2return == 'bestTune'){
print(paste('Best Tune:', m0$bestTune))
return(m0$bestTune)}
}
model_opt(s1.50k.nolimits_ran, 'leapBackward', 11, train.control, 'bestTune')
model_opt(s1.50k.nolimits_ran, 'leapBackward', 11, train.control, 'finalModel')
model_opt(s1.50k.nolimits_ran, 'leapBackward', 11, train.control, 'results')
