p_x <- .993
p_x <- .993
p_x_not <- 1-.993
p_x_not
rm (list = ls())
rm (list = ls())
p_x <- .993
p_x_not <- 1-p_x
p_y <- .997
p_y_not <- 1-p_y
rm (list = ls())
x <-     c(0,0,0,0,0,1,1,1,1,2,2,2,3,3,4)
y <-     c(0,1,2,3,4,0,1,2,3,0,1,2,0,1,0)
length(x)
x_not <- 4-x
x_not
y_not <- 4-x
y_not <- 4-x
rm (list = ls())
x <-     c(0,0,0,0,0,1,1,1,1,2,2,2,3,3,4)
y <-     c(0,1,2,3,4,0,1,2,3,0,1,2,0,1,0)
x_not <- 4-x
y_not <- 4-x
p_x <- .993
p_y <- .997
p_x_not <- 1-p_x
p_y_not <- 1-p_y
fXY <- p_x^x * p_y^y * p_x_not^x_not * p_y_not^y_not
f_XY <- p_x^x * p_y^y * p_x_not^x_not * p_y_not^y_not
f_XY
plto(f_XY)
plot(f_XY)
x_not
.007^0
.003^4
rm (list = ls())
n <- 4
x <-     c(0,0,0,0,0,1,1,1,1,2,2,2,3,3,4)
y <-     c(0,1,2,3,4,0,1,2,3,0,1,2,0,1,0)
x_not <- 4-x
y_not <- 4-x
p_x <- .993
p_y <- .997
p_x_not <- 1-p_x
p_y_not <- 1-p_y
permutations <-  c(factorial(n) / c(factorial(x) * factorial(y) * factorial(n-x-y)))
f_XY <- p_x^x * p_y^y * p_x_not^x_not * p_y_not^y_not
plot(f_XY)
plot(f_XY)
plot(f_XY)
f_XY
permutations
n <- 4
x <-     c(0,0,0,0,0,1,1,1,1,1,2,2,2,2,2,3,3,3,3,3,4,4,4,4,4)
y <-     c(0,1,2,3,4,0,1,2,3,4,0,1,2,3,4,0,1,2,3,4,0,1,2,3,4)
x_not <- 4-x
y_not <- 4-x
p_x <- .993
p_y <- .997
p_x_not <- 1-p_x
p_y_not <- 1-p_y
permutations <-  c(factorial(n) / c(factorial(x) * factorial(y) * factorial(n-x-y)))
f_XY <- p_x^x * p_y^y * p_x_not^x_not * p_y_not^y_not
f_XY
plot(f_XY)
x_not
y_not
y_not <- 4-y
x_not
y_not
rm (list = ls())
n <- 4
x <-     c(0,0,0,0,0,1,1,1,1,1,2,2,2,2,2,3,3,3,3,3,4,4,4,4,4)
y <-     c(0,1,2,3,4,0,1,2,3,4,0,1,2,3,4,0,1,2,3,4,0,1,2,3,4)
x_not <- 4-x
y_not <- 4-y
p_x <- .993
p_y <- .997
p_x_not <- 1-p_x
p_y_not <- 1-p_y
permutations <-  c(factorial(n) / c(factorial(x) * factorial(y) * factorial(n-x-y)))
f_XY <- p_x^x * p_y^y * p_x_not^x_not * p_y_not^y_not
f_XY
plot(f_XY)
permutations
permutations
factorial(0)
4-0-0
permutations <-  c(factorial(n) / c(factorial(x) * factorial(y) * factorial(n-x-y)))
permutations
rm(list= ls())
# Set up --------------------------------------------------------------------------------------------
y <-       c(0,0,0,0,0,1,1,1,1,2,2,2,3,3,4)     # possible values of y
x <-       c(0,1,2,3,4,0,1,2,3,0,1,2,0,1,0)     # possible values of x
n <- 4
permutations <-  c(factorial(n) / c(factorial(x) * factorial(y) * factorial(n-x-y)))
fXY = permutations * p_x^x * p_y^y
y <-       c(0,0,0,0,0,1,1,1,1,2,2,2,3,3,4)     # possible values of y
x <-       c(0,1,2,3,4,0,1,2,3,0,1,2,0,1,0)     # possible values of x
n <- 4
p_x = 0.993
p_y = 0.997
rm(list= ls())
# Set up --------------------------------------------------------------------------------------------
y <-       c(0,0,0,0,0,1,1,1,1,2,2,2,3,3,4)     # possible values of y
x <-       c(0,1,2,3,4,0,1,2,3,0,1,2,0,1,0)     # possible values of x
n <- 4
p_x = 0.993
p_y = 0.997
permutations <-  c(factorial(n) / c(factorial(x) * factorial(y) * factorial(n-x-y)))
fXY = permutations * p_x^x * p_y^y
fXY
rm(list= ls())
# Set up --------------------------------------------------------------------------------------------
y <-       c(0,0,0,0,0,1,1,1,1,2,2,2,3,3,4)     # possible values of y
x <-       c(0,1,2,3,4,0,1,2,3,0,1,2,0,1,0)     # possible values of x
n <- 4
p_x = 0.993*.997
p_y = 1-p_x
permutations <-  c(factorial(n) / c(factorial(x) * factorial(y) * factorial(n-x-y)))
fXY = permutations * p_x^x * p_y^y
fXY
y <-       c(0,1,2,3,4)     # possible values of y
x <-       c(4,3,2,1,0)     # possible values of x
n <- 4
p_x = 0.993*.997
p_y = 1-p_x
permutations <-  c(factorial(n) / c(factorial(x) * factorial(y) * factorial(n-x-y)))
fXY = permutations * p_x^x * p_y^y
fXY
plot(fXY)
fX_4 = sum(fXY[x=4])
fX_3 = sum(fXY[x=3])
fX_2 = sum(fXY[x=2])
fX_1 = sum(fXY[x=1])
fX_0 = sum(fXY[x=0])
fX_4
fX_3
fX_2
fX_1
fX_0
p_x
p_x^4
fX_4 = sum(fXY[x==4])
fX_4 = sum(fXY[x==4])
fX_3 = sum(fXY[x==3])
fX_2 = sum(fXY[x==22])
fX_1 = sum(fXY[x==11])
fX_0 = sum(fXY[x==00])
fX_4
fX_3
fX_2
fX_1
fX_0
plot(c(fX_4,fX_3,fX_2,fX_1,fX_0))
mu_x = c((4*fX_4)+(3*fX_3)+(2*fX_2)+(1*fX_1)+(0*fX_0))
mu_x
rm(list = ls())
## IMPORT LIBRARIES_______________________________________________________________________
library(lattice)
library(ggplot2)
library(caret)  # used for parameter tuning
library(glmnet)
library(pls)
library(ISLR)
## CREATE DATASET_________________________________________________________________________
setwd('/home/ccirelli2/Desktop/Repositories/ML_Final_Project_2019/Gotham_Cabs/data')
s1.50k.nolimits        = read.csv('sample1_50k.csv')[2:12]                          #[2:12] drop datetime col.
s2.100k.nolimits       = read.csv('sample1_100k.csv')[2:12]
s3.250k.nolimits       = read.csv('sample1_250k.csv')[2:12]
s4.50k.wlimits         = read.csv('sample2_wlimits_50k.csv')[2:12]
s5.100k.wlimits        = read.csv('sample2_wlimits_100k.csv')[2:12]
s6.250k.wlimits        = read.csv('sample2_wlimits_250k.csv')[2:12]
# SET SEED FOR ENTIRE CODE________________________________________________________________
set.seed(123)
# RANDOMIZE DATA__________________________________________________________________________
s1.50k.nolimits_ran    = s1.50k.nolimits[sample(nrow(s1.50k.nolimits)),]
s2.100k.nolimits_ran   = s2.100k.nolimits[sample(nrow(s2.100k.nolimits)),]
s3.250k.nolimits_ran   = s3.250k.nolimits[sample(nrow(s3.250k.nolimits)),]
s4.50k.wlimits_ran     = s4.50k.wlimits[sample(nrow(s4.50k.wlimits)), ]
s5.100k.wlimits_ran    = s5.100k.wlimits[sample(nrow(s5.100k.wlimits)), ]
s6.250k.wlimits_ran    = s6.250k.wlimits[sample(nrow(s6.250k.wlimits)), ]
# Calculate Number of Training Observations
train_nrows_50k  = (nrow(s1.50k.nolimits)  * .7)
train_nrows_100k = (nrow(s2.100k.nolimits)   * .7)
train_nrows_250k = (nrow(s3.250k.nolimits)   * .7)
# Train
s1.train = s1.50k.nolimits_ran[1:  (nrow(s1.50k.nolimits_ran)  * .7), ]
s2.train = s2.100k.nolimits_ran[1: (nrow(s2.100k.nolimits_ran) * .7), ]
s3.train = s3.250k.nolimits_ran[1: (nrow(s3.250k.nolimits_ran) * .7), ]
s4.train = s4.50k.wlimits_ran[1:   (nrow(s4.50k.wlimits_ran)  * .7), ]
s5.train = s5.100k.wlimits_ran[1:  (nrow(s5.100k.wlimits_ran)  * .7), ]
s6.train = s6.250k.wlimits_ran[1:  (nrow(s6.250k.wlimits_ran)  * .7), ]
# Test
s1.test = s1.50k.nolimits_ran[ train_nrows_50k:   nrow(s1.50k.nolimits_ran), ] # Index from training to total
s2.test = s2.100k.nolimits_ran[train_nrows_100k:  nrow(s2.100k.nolimits_ran), ]
s3.test = s3.250k.nolimits_ran[train_nrows_250k:  nrow(s3.250k.nolimits_ran), ]
s4.test = s4.50k.wlimits_ran[  train_nrows_50k:   nrow(s4.50k.wlimits_ran), ]
s5.test = s5.100k.wlimits_ran[ train_nrows_100k:  nrow(s5.100k.wlimits_ran), ]
s6.test = s6.250k.wlimits_ran[ train_nrows_250k:  nrow(s6.250k.wlimits_ran), ]
# Separate Target & Feature Values
s1_y = s1.train$duration
s1_x = as.matrix(s1.train[,2:11])
s2_y = s2.train$duration
s2_x = as.matrix(s2.train[,2:11])
s3_y = s3.train$duration
s3_x = as.matrix(s3.train[,2:11])
s4_y = s4.train$duration
s4_x = as.matrix(s4.train[,2:11])
s5_y = s5.train$duration
s5_x = as.matrix(s5.train[,2:11])
s6_y = s6.train$duration
s6_x = as.matrix(s6.train[,2:11])
# Generate Grid Possible Values Lambda
grid = 10^seq(from = 10, to = -2, length = 100)                 #length = desired length of sequence
rse.test = c()
list.lambda    = c()
X.datasets     = list(s1_x, s4_x)
Y.datasets     = list(s1_y, s4_y)
names.datasets = c('s1', 's4')
for (i in seq(1,6)){
# Create Data Objects
print('Creating Datasets')
X = X.datasets[[i]]
Y = Y.datasets[[i]]
# Train Model Using CV
print('Training CV Model')
m_cv = cv.glmnet(X, Y, alpha = 0, lambda = grid, standardize = TRUE, nfolds = 10)
# Get Best Lambda
cv_lambda = m_cv$lambda.min
# Fit Model w/ Best Lambda
print('Fit Model w/ Best Lambda')
m_optimal <- glmnet(X, Y, alpha = 0, lambda = cv_lambda, standardize = TRUE)
# Generate Prediction
print('Generate Prediction')
y_hat_cv <- predict(m_optimal, X)
# Calculate RSE
print('Calculate RSE')
model_cv_rse = sqrt(sum((Y - y_hat_cv)^2) / (length(Y) - 2))
print(paste('Model ', i, 'RSE =>', model_cv_rse))
# Append RSE Values To List
rse.test[i] = model_cv_rse
print(paste('Iteration', i, 'completed'))
}
print('hello world')
for (i in seq(1,2)){
# Create Data Objects
print('Creating Datasets')
X = X.datasets[[i]]
Y = Y.datasets[[i]]
# Train Model Using CV
print('Training CV Model')
m_cv = cv.glmnet(X, Y, alpha = 0, lambda = grid, standardize = TRUE, nfolds = 10)
# Get Best Lambda
cv_lambda = m_cv$lambda.min
# Fit Model w/ Best Lambda
print('Fit Model w/ Best Lambda')
m_optimal <- glmnet(X, Y, alpha = 0, lambda = cv_lambda, standardize = TRUE)
# Generate Prediction
print('Generate Prediction')
y_hat_cv <- predict(m_optimal, X)
# Calculate RSE
print('Calculate RSE')
model_cv_rse = sqrt(sum((Y - y_hat_cv)^2) / (length(Y) - 2))
print(paste('Model ', i, 'RSE =>', model_cv_rse))
# Append RSE Values To List
rse.test[i] = model_cv_rse
print(paste('Iteration', i, 'completed'))
}
rse.test
rse.test = c()
list.lambda    = c()
X.datasets     = list(s1_x, s2_x, s3_x, s4_x, s5_x, s6_x)
Y.datasets     = list(s1_y, s2_y, s3_y, s4_y, s5_y, s6_y)
names.datasets = c('s1', 's2', 's3', 's4', 's5', 's6')
df = data.frame(row.names = names.datasets)
df$ridge.rse = rse.test
df
df$ridge.rse = rse.test
rse.test
rse.test = c()
list.lambda    = c()
X.datasets     = list(s1_x, s2_x, s3_x, s4_x, s5_x, s6_x)
Y.datasets     = list(s1_y, s2_y, s3_y, s4_y, s5_y, s6_y)
names.datasets = c('s1', 's2', 's3', 's4', 's5', 's6')
for (i in seq(1,6)){
# Create Data Objects
print('Creating Datasets')
X = X.datasets[[i]]
Y = Y.datasets[[i]]
# Train Model Using CV
print('Training CV Model')
m_cv = cv.glmnet(X, Y, alpha = 0, lambda = grid, standardize = TRUE, nfolds = 10)
# Get Best Lambda
cv_lambda = m_cv$lambda.min
# Fit Model w/ Best Lambda
print('Fit Model w/ Best Lambda')
m_optimal <- glmnet(X, Y, alpha = 0, lambda = cv_lambda, standardize = TRUE)
# Generate Prediction
print('Generate Prediction')
y_hat_cv <- predict(m_optimal, X)
# Calculate RSE
print('Calculate RSE')
model_cv_rse = sqrt(sum((Y - y_hat_cv)^2) / (length(Y) - 2))
print(paste('Model ', i, 'RSE =>', model_cv_rse))
# Append RSE Values To List
rse.test[i] = round(model_cv_rse,0)
print(paste('Iteration', i, 'completed'))
}
s1_y = s1.50k.nolimits_ran$duration
s1_x = as.matrix(s1.50k.nolimits_ran[,2:11])
s2_y = s2.100k.nolimits_ran$duration
s2_x = as.matrix(s2.100k.nolimits_ran[,2:11])
s3_y = s3.250k.nolimits_ran$duration
s3_x = as.matrix(s3.250k.nolimits_ran[,2:11])
s4_y = s4.50k.wlimits_ran$duration
s4_x = as.matrix(s4.50k.wlimits_ran[,2:11])
s5_y = s5.100k.wlimits_ran$duration
s5_x = as.matrix(s5.100k.wlimits_ran[,2:11])
s6_y = s6.250k.wlimits_ran$duration
s6_x = as.matrix(s6.250k.wlimits_ran[,2:11])
# Generate Grid Possible Values Lambda
grid = 10^seq(from = 10, to = -2, length = 100)                 #length = desired length of sequence
for (i in seq(1,6)){
# Create Data Objects
print('Creating Datasets')
X = X.datasets[[i]]
Y = Y.datasets[[i]]
# Train Model Using CV
print('Training CV Model')
m_cv = cv.glmnet(X, Y, alpha = 0, lambda = grid, standardize = TRUE, nfolds = 10)
# Get Best Lambda
cv_lambda = m_cv$lambda.min
# Fit Model w/ Best Lambda
print('Fit Model w/ Best Lambda')
m_optimal <- glmnet(X, Y, alpha = 0, lambda = cv_lambda, standardize = TRUE)
# Generate Prediction
print('Generate Prediction')
y_hat_cv <- predict(m_optimal, X)
# Calculate RSE
print('Calculate RSE')
model_cv_rse = sqrt(sum((Y - y_hat_cv)^2) / (length(Y) - 2))
print(paste('Model ', i, 'RSE =>', model_cv_rse))
# Append RSE Values To List
rse.test[i] = round(model_cv_rse,0)
print(paste('Iteration', i, 'completed'))
}
X.datasets     = list(s1_x, s3_x, s4_x, s5_x, s6_x)
Y.datasets     = list(s1_y, s3_y, s4_y, s5_y, s6_y)
names.datasets = c('s1','s3', 's4', 's5', 's6')
for (i in seq(1,6)){
# Create Data Objects
print('Creating Datasets')
X = X.datasets[[i]]
Y = Y.datasets[[i]]
# Train Model Using CV
print('Training CV Model')
m_cv = cv.glmnet(X, Y, alpha = 0, lambda = grid, standardize = TRUE, nfolds = 10)
# Get Best Lambda
cv_lambda = m_cv$lambda.min
# Fit Model w/ Best Lambda
print('Fit Model w/ Best Lambda')
m_optimal <- glmnet(X, Y, alpha = 0, lambda = cv_lambda, standardize = TRUE)
# Generate Prediction
print('Generate Prediction')
y_hat_cv <- predict(m_optimal, X)
# Calculate RSE
print('Calculate RSE')
model_cv_rse = sqrt(sum((Y - y_hat_cv)^2) / (length(Y) - 2))
print(paste('Model ', i, 'RSE =>', model_cv_rse))
# Append RSE Values To List
rse.test[i] = round(model_cv_rse,0)
print(paste('Iteration', i, 'completed'))
}
X.datasets     = list(s1_x, s4_x, s5_x, s6_x)
Y.datasets     = list(s1_y, s4_y, s5_y, s6_y)
names.datasets = c('s1','s4', 's5', 's6')
for (i in seq(1,6)){
# Create Data Objects
print('Creating Datasets')
X = X.datasets[[i]]
Y = Y.datasets[[i]]
# Train Model Using CV
print('Training CV Model')
m_cv = cv.glmnet(X, Y, alpha = 0, lambda = grid, standardize = TRUE, nfolds = 10)
# Get Best Lambda
cv_lambda = m_cv$lambda.min
# Fit Model w/ Best Lambda
print('Fit Model w/ Best Lambda')
m_optimal <- glmnet(X, Y, alpha = 0, lambda = cv_lambda, standardize = TRUE)
# Generate Prediction
print('Generate Prediction')
y_hat_cv <- predict(m_optimal, X)
# Calculate RSE
print('Calculate RSE')
model_cv_rse = sqrt(sum((Y - y_hat_cv)^2) / (length(Y) - 2))
print(paste('Model ', i, 'RSE =>', model_cv_rse))
# Append RSE Values To List
rse.test[i] = round(model_cv_rse,0)
print(paste('Iteration', i, 'completed'))
}
rse.test = c()
list.lambda    = c()
X.datasets     = list(s1_x, s4_x, s5_x, s6_x)
Y.datasets     = list(s1_y, s4_y, s5_y, s6_y)
names.datasets = c('s1','s4', 's5', 's6')
for (i in seq(1,4)){
# Create Data Objects
print('Creating Datasets')
X = X.datasets[[i]]
Y = Y.datasets[[i]]
# Train Model Using CV
print('Training CV Model')
m_cv = cv.glmnet(X, Y, alpha = 0, lambda = grid, standardize = TRUE, nfolds = 10)
# Get Best Lambda
cv_lambda = m_cv$lambda.min
# Fit Model w/ Best Lambda
print('Fit Model w/ Best Lambda')
m_optimal <- glmnet(X, Y, alpha = 0, lambda = cv_lambda, standardize = TRUE)
# Generate Prediction
print('Generate Prediction')
y_hat_cv <- predict(m_optimal, X)
# Calculate RSE
print('Calculate RSE')
model_cv_rse = sqrt(sum((Y - y_hat_cv)^2) / (length(Y) - 2))
print(paste('Model ', i, 'RSE =>', model_cv_rse))
# Append RSE Values To List
rse.test[i] = round(model_cv_rse,0)
print(paste('Iteration', i, 'completed'))
}
# Create DataFrame
df = data.frame(row.names = names.datasets)
df$ridge.rse = rse.test
ggplot(df, aes(y = df$ridge.rse, x = names.datasets, fill = names.datasets)) + geom_bar(stat = 'identity') +
ggtitle('Multilinear Regression - RSE Over 6 Datasets') +
scale_y_continuous(breaks = pretty(df$ridge.rse, n = 5))
names.datasets = c('50knl','50kwl', '100kwl', '250kwl')
ggplot(df, aes(y = df$ridge.rse, x = names.datasets, fill = names.datasets)) + geom_bar(stat = 'identity') +
ggtitle('Multilinear Regression - RSE Over 6 Datasets') +
scale_y_continuous(breaks = pretty(df$ridge.rse, n = 5))
ggplot(df, aes(y = df$ridge.rse, x = names.datasets, fill = names.datasets)) + geom_bar(stat = 'identity') +
ggtitle('Multilinear Ridge Regression - 4 Datasets - RSE') +
scale_y_continuous(breaks = pretty(df$ridge.rse, n = 5))
ggplot(data = df, aes(x = seq(1,100), y = df$sumcoeff), fill = df) + geom_line() + ggtitle('Ridge - Sum of Squared Coefficeints For Each Lambda')
# Define Lists to Capture Output
rse.test = c()
list.lambda    = c()
X.datasets     = list(s1_x, s4_x, s5_x, s6_x)
Y.datasets     = list(s1_y, s4_y, s5_y, s6_y)
names.datasets = c('50knl','50kwl', '100kwl', '250kwl')
for (i in seq(1,4)){
# Create Data Objects
print('Creating Datasets')
X = X.datasets[[i]]
Y = Y.datasets[[i]]
# Train Model Using CV
print('Training CV Model')
m_cv = cv.glmnet(X, Y, alpha = 1, lambda = grid, standardize = TRUE, nfolds = 10)
# Get Best Lambda
cv_lambda = m_cv$lambda.min
# Fit Model w/ Best Lambda
print('Fit Model w/ Best Lambda')
m_optimal <- glmnet(X, Y, alpha = 1, lambda = cv_lambda, standardize = TRUE)
# Generate Prediction
print('Generate Prediction')
y_hat_cv <- predict(m_optimal, X)
# Calculate RSE
print('Calculate RSE')
model_cv_rse = sqrt(sum((Y - y_hat_cv)^2) / (length(Y) - 2))
print(paste('Model ', i, 'RSE =>', model_cv_rse))
# Append RSE Values To List
rse.test[i] = round(model_cv_rse,0)
print(paste('Iteration', i, 'completed'))
}
df = data.frame(row.names = names.datasets)
df$ridge.rse = rse.test
ggplot(df, aes(y = df$ridge.rse, x = names.datasets, fill = names.datasets)) + geom_bar(stat = 'identity') +
ggtitle('Multilinear Lasso Regression - 4 Datasets - RSE') +
scale_y_continuous(breaks = pretty(df$ridge.rse, n = 5))
df
for (i in seq(1,4)){
# Create Data Objects
print('Creating Datasets')
X = X.datasets[[i]]
Y = Y.datasets[[i]]
# Train Model Using CV
print('Training CV Model')
m_cv = cv.glmnet(X, Y, alpha = 0, lambda = grid, standardize = TRUE, nfolds = 10)
# Get Best Lambda
cv_lambda = m_cv$lambda.min
# Fit Model w/ Best Lambda
print('Fit Model w/ Best Lambda')
m_optimal <- glmnet(X, Y, alpha = 0, lambda = cv_lambda, standardize = TRUE)
# Generate Prediction
print('Generate Prediction')
y_hat_cv <- predict(m_optimal, X)
# Calculate RSE
print('Calculate RSE')
model_cv_rse = sqrt(sum((Y - y_hat_cv)^2) / (length(Y) - 2))
print(paste('Model ', i, 'RSE =>', model_cv_rse))
# Append RSE Values To List
rse.test[i] = round(model_cv_rse,0)
print(paste('Iteration', i, 'completed'))
}
# Create DataFrame
df = data.frame(row.names = names.datasets)
df$ridge.rse = rse.test
# Generate a Plot for Train & Test Points
ggplot(df, aes(y = df$ridge.rse, x = names.datasets, fill = names.datasets)) + geom_bar(stat = 'identity') +
ggtitle('Multilinear Lasso Regression - 4 Datasets - RSE') +
scale_y_continuous(breaks = pretty(df$ridge.rse, n = 5))
df
