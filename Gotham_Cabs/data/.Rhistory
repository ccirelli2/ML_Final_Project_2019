train_nrows_100k = (nrow(s2.100k.nolimits)   * .7)
train_nrows_250k = (nrow(s3.250k.nolimits)   * .7)
# Train
s1.train = s1.50k.nolimits_ran[1:  (nrow(s1.50k.nolimits_ran)  * .7), ]
s2.train = s2.100k.nolimits_ran[1: (nrow(s2.100k.nolimits_ran) * .7), ]
s3.train = s3.250k.nolimits_ran[1: (nrow(s3.250k.nolimits_ran) * .7), ]
s4.train = s4.50k.wlimits_ran[1:   (nrow(s4.50k.wlimits_ran)  * .7), ]
s5.train = s5.100k.wlimits_ran[1:  (nrow(s5.100k.wlimits_ran)  * .7), ]
s6.train = s6.250k.wlimits_ran[1:  (nrow(s6.250k.wlimits_ran)  * .7), ]
# Test
s1.test = s1.50k.nolimits_ran[ train_nrows_50k:   nrow(s1.50k.nolimits_ran), ] # Index from training to total
s2.test = s2.100k.nolimits_ran[train_nrows_100k:  nrow(s2.100k.nolimits_ran), ]
s3.test = s3.250k.nolimits_ran[train_nrows_250k:  nrow(s3.250k.nolimits_ran), ]
s4.test = s4.50k.wlimits_ran[  train_nrows_50k:   nrow(s4.50k.wlimits_ran), ]
s5.test = s5.100k.wlimits_ran[ train_nrows_100k:  nrow(s5.100k.wlimits_ran), ]
s6.test = s6.250k.wlimits_ran[ train_nrows_250k:  nrow(s6.250k.wlimits_ran), ]
# Separate Target & Feature Values
s1_y = s1.50k.nolimits_ran$duration
s1_x = as.matrix(s1.50k.nolimits_ran[,2:10])
s2_y = s2.100k.nolimits_ran$duration
s2_x = as.matrix(s2.100k.nolimits_ran[,2:10])
s3_y = s3.250k.nolimits_ran$duration
s3_x = as.matrix(s3.250k.nolimits_ran[,2:10])
s4_y = s4.50k.wlimits_ran$duration
s4_x = as.matrix(s4.50k.wlimits_ran[,2:10])
s5_y = s5.100k.wlimits_ran$duration
s5_x = as.matrix(s5.100k.wlimits_ran[,2:10])
s6_y = s6.250k.wlimits_ran$duration
s6_x = as.matrix(s6.250k.wlimits_ran[,2:10])
# Generate Grid Possible Values Lambda
grid = 10^seq(from = 10, to = -2, length = 100)                 #length = desired length of sequence
# Train Model
m1.ridge <- glmnet(s1_x, s1_y, alpha = 0, lambda = grid, standardize = TRUE)
# Create List to Capture Sum of Coefficients
list_sum_ceoffs <- c()
df = data.frame(row.names = grid)
for (i in seq(1,length(m1.ridge$lambda))){
sum_coeff = sum((sqrt(coef(m1.ridge)[,i]^2)))
list_sum_ceoffs[i] = sum_coeff
}
df$sumcoeff = list_sum_ceoffs
ggplot(data = df, aes(x = seq(1,100), y = df$sumcoeff)) + geom_line() + ggtitle('Ridge - Sum of Squared Coefficeints For Each Lambda')
rse.test = c()
list.lambda    = c()
X.datasets     = list(s1_x, s4_x, s5_x, s6_x)
Y.datasets     = list(s1_y, s4_y, s5_y, s6_y)
names.datasets = c('50knl','50kwl', '100kwl', '250kwl')
for (i in seq(1,4)){
# Create Data Objects
print('Creating Datasets')
X = X.datasets[[i]]
Y = Y.datasets[[i]]
# Train Model Using CV
print('Training CV Model')
m_cv = cv.glmnet(X, Y, alpha = 0, lambda = grid, standardize = TRUE, nfolds = 10)
# Get Best Lambda
cv_lambda = m_cv$lambda.min
# Fit Model w/ Best Lambda
print('Fit Model w/ Best Lambda')
m_optimal <- glmnet(X, Y, alpha = 0, lambda = cv_lambda, standardize = TRUE)
# Generate Prediction
print('Generate Prediction')
y_hat_cv <- predict(m_optimal, X)
# Calculate RSE
print('Calculate RSE')
model_cv_rse = sqrt(sum((Y - y_hat_cv)^2) / (length(Y) - 2))
print(paste('Model ', i, 'RSE =>', model_cv_rse))
# Append RSE Values To List
rse.test[i] = round(model_cv_rse,0)
print(paste('Iteration', i, 'completed'))
}
df = data.frame(row.names = names.datasets)
df$ridge.rse = rse.test
ggplot(df, aes(y = df$ridge.rse, x = names.datasets, fill = names.datasets)) + geom_bar(stat = 'identity') +
ggtitle('Multilinear Lasso Regression - 4 Datasets - RSE') +
scale_y_continuous(breaks = pretty(df$ridge.rse, n = 5))
df
ggplot(data = df, aes(x = seq(1,100), y = df$sumcoeff)) + geom_line() + ggtitle('Ridge - Sum of Squared Coefficeints For Each Lambda') +
xlab('Datasets') + ylab('Test RSE')
# Create List to Capture Sum of Coefficients
list_sum_ceoffs <- c()
df = data.frame(row.names = grid)
df
# Train Model
m1.ridge <- glmnet(s1_x, s1_y, alpha = 0, lambda = grid, standardize = TRUE)
# Create List to Capture Sum of Coefficients
list_sum_ceoffs <- c()
df = data.frame(row.names = grid)
for (i in seq(1,length(m1.ridge$lambda))){
sum_coeff = sum((sqrt(coef(m1.ridge)[,i]^2)))
list_sum_ceoffs[i] = sum_coeff
}
rse.test = c()
list.lambda    = c()
X.datasets     = list(s1_x, s4_x, s5_x, s6_x)
Y.datasets     = list(s1_y, s4_y, s5_y, s6_y)
names.datasets = c('50knl','50kwl', '100kwl', '250kwl')
for (i in seq(1,4)){
# Create Data Objects
print('Creating Datasets')
X = X.datasets[[i]]
Y = Y.datasets[[i]]
# Train Model Using CV
print('Training CV Model')
m_cv = cv.glmnet(X, Y, alpha = 0, lambda = grid, standardize = TRUE, nfolds = 10)
# Get Best Lambda
cv_lambda = m_cv$lambda.min
# Fit Model w/ Best Lambda
print('Fit Model w/ Best Lambda')
m_optimal <- glmnet(X, Y, alpha = 0, lambda = cv_lambda, standardize = TRUE)
# Generate Prediction
print('Generate Prediction')
y_hat_cv <- predict(m_optimal, X)
# Calculate RSE
print('Calculate RSE')
model_cv_rse = sqrt(sum((Y - y_hat_cv)^2) / (length(Y) - 2))
print(paste('Model ', i, 'RSE =>', model_cv_rse))
# Append RSE Values To List
rse.test[i] = round(model_cv_rse,0)
print(paste('Iteration', i, 'completed'))
}
for (i in seq(1,4)){
# Create Data Objects
print('Creating Datasets')
X = X.datasets[[i]]
Y = Y.datasets[[i]]
# Train Model Using CV
print('Training CV Model')
m_cv = cv.glmnet(X, Y, alpha = 0, lambda = grid, standardize = TRUE, nfolds = 10)
# Get Best Lambda
cv_lambda = m_cv$lambda.min
print(paste('Best lambda =>', cv_lambda))
# Fit Model w/ Best Lambda
print('Fit Model w/ Best Lambda')
m_optimal <- glmnet(X, Y, alpha = 0, lambda = cv_lambda, standardize = TRUE)
# Generate Prediction
print('Generate Prediction')
y_hat_cv <- predict(m_optimal, X)
# Calculate RSE
print('Calculate RSE')
model_cv_rse = sqrt(sum((Y - y_hat_cv)^2) / (length(Y) - 2))
print(paste('Model ', i, ' TEST RSE =>', model_cv_rse))
# Append RSE Values To List
rse.test[i] = round(model_cv_rse,0)
print(paste('Iteration', i, 'completed'))
print('--------------------------------------------------------------------------')
}
# Create DataFrame
df = data.frame(row.names = names.datasets)
df$ridge.rse = rse.test
ggplot(df, aes(y = df$ridge.rse, x = names.datasets, fill = names.datasets)) + geom_bar(stat = 'identity') +
ggtitle('Multilinear Lasso Regression - 4 Datasets - RSE') +
scale_y_continuous(breaks = pretty(df$ridge.rse, n = 5)) + xlab('Datasets') + ylab('TEST RSE')
# Define Lists to Capture Output
rse.test = c()
list.lambda    = c()
X.datasets     = list(s1_x, s4_x, s5_x, s6_x)
Y.datasets     = list(s1_y, s4_y, s5_y, s6_y)
names.datasets = c('50knl','50kwl', '100kwl', '250kwl')
for (i in seq(1,4)){
# Create Data Objects
print('Creating Datasets')
X = X.datasets[[i]]
Y = Y.datasets[[i]]
# Train Model Using CV
print('Training CV Model')
m_cv = cv.glmnet(X, Y, alpha = 1, lambda = grid, standardize = TRUE, nfolds = 10)
# Get Best Lambda
cv_lambda = m_cv$lambda.min
print(paste('Best lambda =>', cv_lambda))
# Fit Model w/ Best Lambda
print('Fit Model w/ Best Lambda')
m_optimal <- glmnet(X, Y, alpha = 0, lambda = cv_lambda, standardize = TRUE)
# Generate Prediction
print('Generate Prediction')
y_hat_cv <- predict(m_optimal, X)
# Calculate RSE
print('Calculate RSE')
model_cv_rse = sqrt(sum((Y - y_hat_cv)^2) / (length(Y) - 2))
print(paste('Model ', i, ' TEST RSE =>', model_cv_rse))
# Append RSE Values To List
rse.test[i] = round(model_cv_rse,0)
print(paste('Iteration', i, 'completed'))
print('--------------------------------------------------------------------------')
}
?cv.glmnet
# Create DataFrame
df = data.frame(row.names = names.datasets)
df$ridge.rse = rse.test
ggplot(df, aes(y = df$ridge.rse, x = names.datasets, fill = names.datasets)) + geom_bar(stat = 'identity') +
ggtitle('Multilinear Lasso Regression - 4 Datasets - RSE') +
scale_y_continuous(breaks = pretty(df$ridge.rse, n = 5)) + xlab('Datasets') + ylab('TEST RSE')
## CLEAR NAMESPACE________________________________________________________________________
rm(list = ls())
## IMPORT LIBRARIES_______________________________________________________________________
library(lattice)
library(ggplot2)
library(caret)  # used for parameter tuning
library(glmnet)
library(pls)
library(ISLR)
setwd('/home/ccirelli2/Desktop/Repositories/ML_Final_Project_2019/Gotham_Cabs/data')
s1.50k.nolimits        = read.csv('sample1_50k.csv')[2:12]                          #[2:12] drop datetime col.
s2.100k.nolimits       = read.csv('sample1_100k.csv')[2:12]
s3.250k.nolimits       = read.csv('sample1_250k.csv')[2:12]
s4.50k.wlimits         = read.csv('sample2_wlimits_50k.csv')[2:12]
s5.100k.wlimits        = read.csv('sample2_wlimits_100k.csv')[2:12]
s6.250k.wlimits        = read.csv('sample2_wlimits_250k.csv')[2:12]
## DROP SPEED_____________________________________________________________________________
s1.50k.nolimits$speed  <- NULL
s2.100k.nolimits$speed <- NULL
s3.250k.nolimits$speed <- NULL
s4.50k.wlimits$speed   <- NULL
s5.100k.wlimits$speed  <- NULL
s6.250k.wlimits$speed  <- NULL
# SET SEED FOR ENTIRE CODE________________________________________________________________
set.seed(123)
s1.50k.nolimits_ran    = s1.50k.nolimits[sample(nrow(s1.50k.nolimits)),]
s2.100k.nolimits_ran   = s2.100k.nolimits[sample(nrow(s2.100k.nolimits)),]
s3.250k.nolimits_ran   = s3.250k.nolimits[sample(nrow(s3.250k.nolimits)),]
s4.50k.wlimits_ran     = s4.50k.wlimits[sample(nrow(s4.50k.wlimits)), ]
s5.100k.wlimits_ran    = s5.100k.wlimits[sample(nrow(s5.100k.wlimits)), ]
s6.250k.wlimits_ran    = s6.250k.wlimits[sample(nrow(s6.250k.wlimits)), ]
# Define Train Control Object
train.control = trainControl(method = 'cv', number = 10)
m6.forward = train(duration ~ ., data = s6.250k.wlimits_ran,
method = 'leapForward',
tuneGrid = data.frame(nvmax = 2:11),
trControl = train.control)
# Get Results
m6.summary = summary(m6.forward$finalModel)
m6.forward$bestTune
m6.forward = train(duration ~ ., data = s6.250k.wlimits_ran,
method = 'leapForward',
tuneGrid = data.frame(nvmax = 2:11),
trControl = train.control)
# Get Results
m6.summary = summary(m6.forward$finalModel)
m6.forward = train(duration ~ ., data = s6.250k.wlimits_ran,
method = 'leapForward',
tuneGrid = data.frame(nvmax = 2:11),
trControl = train.control)
# FORWARD SELECTION ----------------------------------------------------------------------
m6.forward = train(duration ~ ., data = s6.250k.wlimits_ran,
method = 'leapForward',
tuneGrid = data.frame(nvmax = 2:10),
trControl = train.control)
print('hello world')
# Get Results
m6.summary = summary(m6.forward$finalModel)
m6.forward$bestTune
# Plot Results RMSE vs Number of Features
plot(m6.forward$results$RMSE, main = 'MLR - Forward Selection - RMSE', xlab = 'Number of Features')
# Create Training Method - CV & 10kfold
train.control = trainControl(method = 'cv', number = 10)
model_opt <- function(dataset, opt_method, num_param, cv_grid, result2return){
'opt_method:        either leapBackward or leapForward
results:           A dataframe with the training error rate and values for the tuning params
bestTune           A dataframe with the final parameters
finalModel         Aa fit object using the best parameters'
# Train Model
m0 = train(duration ~ dataset$pickup_x, dataset$pickup_y, dataset$dropoff_x, dataset$dropoff_y,
dataset$weekday, dataset$hour_, dataset$day_, dataset$distance, dataset$month_,
data      = dataset,
method    = opt_method,                                   # Step selection
tuneGrid  = data.frame(nvmax = 1 : num_param),            # Number of features to consider in the model
trControl = cv_grid)                                      # Cross Validation technique
# Results
if (result2return == 'results'){
print(paste('Results:',m0$results))
return(m0$results)}
# Final Model - # An asterisk specifies that the feature was included in the model
else if (result2return == 'finalModel'){
print(summary(m0$finalModel))
return(summary(m1.backward$finalModel))}
else if (result2return == 'bestTune'){
print(paste('Best Tune:', m0$bestTune))
return(m0$bestTune)}
}
m0.output = model_opt(s4.50k.wlimits_ran, 'leapBackward', 9, train.control, 'bestTune')
param_index = seq(from = 1, to = 11, by = 1)
m0.output$RMSE
# Create Training Method - CV & 10kfold
train.control = trainControl(method = 'cv', number = 10)
model_opt <- function(dataset, opt_method, num_param, cv_grid, result2return){
'opt_method:        either leapBackward or leapForward
results:           A dataframe with the training error rate and values for the tuning params
bestTune           A dataframe with the final parameters
finalModel         Aa fit object using the best parameters'
# Train Model
m0 = train(duration ~ dataset$pickup_x, dataset$pickup_y, dataset$dropoff_x, dataset$dropoff_y,
dataset$weekday, dataset$hour_, dataset$day_, dataset$distance, dataset$month_,
data      = dataset,
method    = opt_method,                                   # Step selection
tuneGrid  = data.frame(nvmax = 1 : num_param),            # Number of features to consider in the model
trControl = cv_grid)                                      # Cross Validation technique
# Results
if (result2return == 'results'){
print(paste('Results:',m0$results))
return(m0$results)}
# Final Model - # An asterisk specifies that the feature was included in the model
else if (result2return == 'finalModel'){
print(summary(m0$finalModel))
return(summary(m1.backward$finalModel))}
else if (result2return == 'bestTune'){
print(paste('Best Tune:', m0$bestTune))
return(m0$bestTune)}
}
m0.output = model_opt(s4.50k.wlimits_ran, 'leapBackward', 9, train.control, 'bestTune')
m0.output = model_opt(s4.50k.wlimits_ran, 'leapforward', 9, train.control, 'bestTune')
m0.output = model_opt(s4.50k.wlimits_ran, 'leapForward', 9, train.control, 'bestTune')
# RANDOMIZE DATA__________________________________________________________________________
s1.50k.nolimits_ran    = s1.50k.nolimits[sample(nrow(s1.50k.nolimits)),]
s2.100k.nolimits_ran   = s2.100k.nolimits[sample(nrow(s2.100k.nolimits)),]
s3.250k.nolimits_ran   = s3.250k.nolimits[sample(nrow(s3.250k.nolimits)),]
s4.50k.wlimits_ran     = s4.50k.wlimits[sample(nrow(s4.50k.wlimits)), ]
s5.100k.wlimits_ran    = s5.100k.wlimits[sample(nrow(s5.100k.wlimits)), ]
s6.250k.wlimits_ran    = s6.250k.wlimits[sample(nrow(s6.250k.wlimits)), ]
# Define Train Control Object
train.control = trainControl(method = 'cv', number = 10)
# Create Training Method - CV & 10kfold
train.control = trainControl(method = 'cv', number = 10)
m0.output = model_opt(s4.50k.wlimits_ran, 'leapForward', 9, train.control, 'bestTune')
# Create Training Method - CV & 10kfold
train.control = trainControl(method = 'cv', number = 10)
model_opt <- function(dataset, opt_method, num_param, cv_grid, result2return){
'opt_method:        either leapBackward or leapForward
results:           A dataframe with the training error rate and values for the tuning params
bestTune           A dataframe with the final parameters
finalModel         Aa fit object using the best parameters'
# Train Model
m0 = train(duration ~ dataset$pickup_x, dataset$pickup_y, dataset$dropoff_x, dataset$dropoff_y,
dataset$weekday, dataset$hour_, dataset$day_, dataset$distance, dataset$month_,
data      = dataset,
method    = opt_method,                                   # Step selection
tuneGrid  = data.frame(nvmax = 1 : num_param),            # Number of features to consider in the model
trControl = cv_grid)                                      # Cross Validation technique
# Results
if (result2return == 'results'){
print(paste('Results:',m0$results))
return(m0$results)}
# Final Model - # An asterisk specifies that the feature was included in the model
else if (result2return == 'finalModel'){
print(summary(m0$finalModel))
return(summary(m1.backward$finalModel))}
else if (result2return == 'bestTune'){
print(paste('Best Tune:', m0$bestTune))
return(m0$bestTune)}
}
m0.output = model_opt(s4.50k.wlimits_ran, 'leapForward', 9, train.control, 'bestTune')
m7.backward = train(duration ~ ., data = s6.250k.wlimits_ran,
method = 'leapBackward',
tuneGrid = data.frame(nvmax = 1:11),
trControl = train.control)
m7.backward
m7.backward$results
summary(m7.backward$finalModel)
m7.backward = train(duration ~ ., data = s6.250k.wlimits_ran,
method = 'leapBackward',
tuneGrid = data.frame(nvmax = 1:11),
trControl = train.control)
## CLEAR NAMESPACE________________________________________________________________________
rm(list = ls())
library(lattice)
library(ggplot2)
library(caret)  # used for parameter tuning
library(glmnet)
library(pls)
library(ISLR)
setwd('/home/ccirelli2/Desktop/Repositories/ML_Final_Project_2019/Gotham_Cabs/data')
s1.50k.nolimits        = read.csv('sample1_50k.csv')[2:12]                          #[2:12] drop datetime col.
s2.100k.nolimits       = read.csv('sample1_100k.csv')[2:12]
s3.250k.nolimits       = read.csv('sample1_250k.csv')[2:12]
s4.50k.wlimits         = read.csv('sample2_wlimits_50k.csv')[2:12]
s5.100k.wlimits        = read.csv('sample2_wlimits_100k.csv')[2:12]
s6.250k.wlimits        = read.csv('sample2_wlimits_250k.csv')[2:12]
## DROP SPEED_____________________________________________________________________________
s1.50k.nolimits$speed  <- NULL
s2.100k.nolimits$speed <- NULL
s3.250k.nolimits$speed <- NULL
s4.50k.wlimits$speed   <- NULL
s5.100k.wlimits$speed  <- NULL
s6.250k.wlimits$speed  <- NULL
# SET SEED FOR ENTIRE CODE________________________________________________________________
set.seed(123)
s1.50k.nolimits_ran    = s1.50k.nolimits[sample(nrow(s1.50k.nolimits)),]
s2.100k.nolimits_ran   = s2.100k.nolimits[sample(nrow(s2.100k.nolimits)),]
s3.250k.nolimits_ran   = s3.250k.nolimits[sample(nrow(s3.250k.nolimits)),]
s4.50k.wlimits_ran     = s4.50k.wlimits[sample(nrow(s4.50k.wlimits)), ]
s5.100k.wlimits_ran    = s5.100k.wlimits[sample(nrow(s5.100k.wlimits)), ]
s6.250k.wlimits_ran    = s6.250k.wlimits[sample(nrow(s6.250k.wlimits)), ]
# Define Train Control Object
train.control = trainControl(method = 'cv', number = 10)
m6.forward = train(duration ~ ., data = s6.250k.wlimits_ran,
method = 'leapForward',
tuneGrid = data.frame(nvmax = 2:9),
trControl = train.control)
print('hello world')
# Get Results
m6.summary = summary(m6.forward$finalModel)
m6.forward$bestTune
# Plot Results RMSE vs Number of Features
plot(m6.forward$results$RMSE, main = 'MLR - Forward Selection - RMSE', xlab = 'Number of Features')
m6.summary
m6.forward = train(duration ~ ., data = s6.250k.wlimits_ran,
method = 'leapBackward',
tuneGrid = data.frame(nvmax = 2:9),
trControl = train.control)
print('hello world')
# Get Results
m6.summary = summary(m6.forward$finalModel)
m6.forward$bestTune
# Plot Results RMSE vs Number of Features
plot(m6.forward$results$RMSE, main = 'MLR - Forward Selection - RMSE', xlab = 'Number of Features')
m6.forward$bestTune
m6.summary
m6.summary$rsq
m6.summary$rss
m6.summary$cp
m6.summary$rss
m6.summary$rss/length(s6.250k.wlimits_ran)
m6.summary$rss/length(s6.250k.wlimits_ran)
m6.summary$rss/length(s6.250k.wlimits_ran$duration)
sqrt(m6.summary$rss/length(s6.250k.wlimits_ran$duration))
m7.backward = train(duration ~ ., data = s6.250k.wlimits_ran,
method = 'leapForward',
tuneGrid = data.frame(nvmax = 1:9),
trControl = train.control)
m7.backward
model.names = c('SLR', 'MLR', 'Poly', 'L1L2', 'Stepwise')
model.rse   = c(346.6, 336.2, 356.8, 337.9, 339.9)
# Create DataFrame
df = data.frame(row.names = model.names)
df$RSE = model.rse
ggplot(df, aes(y = df$RSE, x = reorder(model.names, -df$RSE), fill = model.names)) +
geom_bar(stat = 'identity') +
ggtitle('COMPARISON REGRESSION APPROACHS')  +
scale_y_continuous(breaks = pretty(df$RSE, n = 5))
ggplot(df, aes(y = df$RSE, x = reorder(model.names, -df$RSE), fill = model.names)) +
geom_bar(stat = 'identity') +
ggtitle('COMPARISON REGRESSION APPROACHS')  + xlab('MODELS') + ylab('RMSE')
scale_y_continuous(breaks = pretty(df$RSE, n = 5))
ggplot(df, aes(y = df$RSE, x = reorder(model.names, -df$RSE), fill = model.names)) +
geom_bar(stat = 'identity') +
ggtitle('COMPARISON REGRESSION APPROACHS')  + xlab('MODELS') + ylab('RMSE') +
scale_y_continuous(breaks = pretty(df$RSE, n = 5))
## CLEAR NAMESPACE________________________________________________________________________
rm(list = ls())
## IMPORT LIBRARIES_______________________________________________________________________
library(rpart)
library(rpart.plot)
library(tree)
library(ggplot2)
# LOAD LIBRARIES
library(RMySQL)
mydb <- dbConnect(RMySQL::MySQL(), user='ccirelli2',
password='Work4starr', dbname='GSU',
host = "127.0.0.1")
setwd('/home/ccirelli2/Desktop/Repositories/ML_Final_Project_2019/Gotham_Cabs/data')
s1.50k.nolimits        = read.csv('sample1_50k.csv')[2:12]                          #[2:12] drop datetime col.
s2.100k.nolimits       = read.csv('sample1_100k.csv')[2:12]
s3.250k.nolimits       = read.csv('sample1_250k.csv')[2:12]
s4.50k.wlimits         = read.csv('sample2_wlimits_50k.csv')[2:12]
s5.100k.wlimits        = read.csv('sample2_wlimits_100k.csv')[2:12]
s6.250k.wlimits        = read.csv('sample2_wlimits_250k.csv')[2:12]
s1.50k.nolimits_ran    = s1.50k.nolimits[sample(nrow(s1.50k.nolimits)),]
s2.100k.nolimits_ran   = s2.100k.nolimits[sample(nrow(s2.100k.nolimits)),]
s3.250k.nolimits_ran   = s3.250k.nolimits[sample(nrow(s3.250k.nolimits)),]
s4.50k.wlimits_ran     = s4.50k.wlimits[sample(nrow(s4.50k.wlimits)), ]
s5.100k.wlimits_ran    = s5.100k.wlimits[sample(nrow(s5.100k.wlimits)), ]
s6.250k.wlimits_ran    = s6.250k.wlimits[sample(nrow(s6.250k.wlimits)), ]
## DROP SPEED_____________________________________________________________________________
s1.50k.nolimits$speed  <- NULL
s2.100k.nolimits$speed <- NULL
s3.250k.nolimits$speed <- NULL
s4.50k.wlimits$speed   <- NULL
s5.100k.wlimits$speed  <- NULL
s6.250k.wlimits$speed  <- NULL
# Calculate Number of Training Observations
train_nrows_50k  = (nrow(s1.50k.nolimits)  * .7)
train_nrows_100k = (nrow(s2.100k.nolimits)   * .7)
train_nrows_250k = (nrow(s3.250k.nolimits)   * .7)
# Train
s1.train = s1.50k.nolimits_ran[1:   train_nrows_50k, ]
s2.train = s2.100k.nolimits_ran[1:  train_nrows_100k, ]
s3.train = s3.250k.nolimits_ran[1:  train_nrows_250k, ]
s4.train = s4.50k.wlimits_ran[1:    train_nrows_50k, ]
s5.train = s5.100k.wlimits_ran[1:   train_nrows_100k, ]
s6.train = s6.250k.wlimits_ran[1:   train_nrows_250k, ]
# Test
s1.test = s1.50k.nolimits_ran[train_nrows_50k:    nrow(s1.50k.nolimits_ran), ] # Index from training to total
s2.test = s2.100k.nolimits_ran[train_nrows_100k:  nrow(s2.100k.nolimits_ran), ]
s3.test = s3.250k.nolimits_ran[train_nrows_250k:  nrow(s3.250k.nolimits_ran), ]
s4.test = s4.50k.wlimits_ran[train_nrows_50k:     nrow(s4.50k.wlimits_ran), ]
s5.test = s5.100k.wlimits_ran[train_nrows_100k:   nrow(s5.100k.wlimits_ran), ]
s6.test = s6.250k.wlimits_ran[train_nrows_250k:   nrow(s6.250k.wlimits_ran), ]
query1_alldata = dbSendQuery(mydb, 'SELECT
*
FROM GSU.ML_FinProj_GothamCab_Train
WHERE speed IS NOT NULL
AND duration != 0
AND duration < 5000
AND speed < 1000
ORDER BY RAND()
LIMIT 500000;')
data.random.sample = fetch(query1_alldata, n = -1)
s7.test = data.random.sample[,2:11]
m1.train = rpart(duration ~ ., data = s6.train, method = 'anova')
rpart.plot(m1.train, type = 3, extra = 101, fallen.leaves = T, main = 'Regression Tree - M1')
m1.residuals = residuals(m1.train)
m1.train.rse = sqrt(sum(m1.residuals^2) / (length(m1.residuals) - 2))
print(m1.train.rse)
