s1_x = as.matrix(s1.train[,2:11])
s2_y = s4.train$duration
s2_x = as.matrix(s4.train[,2:11])
s3_y = s2.train$duration
s3_x = as.matrix(s2.train[,2:11])
s4_y = s5.train$duration
s4_x = as.matrix(s5.train[,2:11])
# Generate Grid Possible Values Lambda
grid = 10^seq(from = 10, to = -2, length = 100)                 #length = desired length of sequence
# Train MLR Using All Lambdas Grid
m2.ridge <- glmnet(s1_x, s1_y, alpha = 0, lambda = grid, standardize = TRUE)
plot(m2.ridge)
# Train MLR Using All Lambdas Grid
m2.ridge <- glmnet(s1_x, s1_y, alpha = 1, lambda = grid, standardize = TRUE)
plot(m2.ridge)
plot(m2.ridge, main = 'L1 Norm')
# Train MLR Using All Lambdas Grid
m2.ridge <- glmnet(s1_x, s1_y, alpha = 0, lambda = grid, standardize = TRUE)
plot(m2.ridge, main = 'L2 Norm')
plot(m2.ridge, main = 'L1 Norm')
# Train MLR Using All Lambdas Grid
m2.ridge <- glmnet(s1_x, s1_y, alpha = 0, lambda = grid, standardize = TRUE)
plot(m2.ridge, main = 'L1 Norm')
# Train MLR Using All Lambdas Grid
m2.ridge <- glmnet(s1_x, s1_y, alpha = 0, lambda = grid, standardize = TRUE)
plot(m2.ridge, main = 'L1 Norm')
# Train MLR Using All Lambdas Grid
m2.ridge <- glmnet(s1_x, s1_y, alpha = 1, lambda = grid, standardize = TRUE)
plot(m2.ridge, main = 'L1 Norm')
plot(m2.ridge, main = 'L1 Norm')
# Train MLR Using All Lambdas Grid
m2.ridge <- glmnet(s1_x, s1_y, alpha = 0, lambda = grid, standardize = TRUE)
plot(m2.ridge, main = 'L1 Norm')
library(boot)
rm(list = ls())
graphics.off()
library(ISLR)
library(boot)
# LOOCV for mpg auto problem
glm.fit=glm(mpg~horsepower ,data=Auto)
coef(glm.fit)
summary(glm.fit)
# the default is LOOCV
cv.err=cv.glm(Auto,glm.fit)
print(cv.err$delta) #delta is a vector of two component where the first one is the raw estimate and second one is unbiased
# Now doing a 10 fold CV:
set.seed(17)
cv.error.10=rep(0,5)
for (i in 1:5){
glm.fit=glm(mpg~poly(horsepower ,i),data=Auto)
cv.error.10[i]=cv.glm(Auto,glm.fit,K=10)$delta[1]
}
print(cv.error.10)# This is the value of CV_k
## CLEAR NAMESPACE________________________________________________________________________
rm(list = ls())
## IMPORT LIBRARIES_______________________________________________________________________
library(lattice)
library(ggplot2)
library(caret)  # used for parameter tuning
library(glmnet)
library(pls)
library(ISLR)
library(boot)
## CREATE DATASET_________________________________________________________________________
setwd('/home/ccirelli2/Desktop/Repositories/ML_Final_Project_2019/Gotham_Cabs/data')
s1.50k.nolimits        = read.csv('sample1_50k.csv')[2:12]                          #[2:12] drop datetime col.
s2.100k.nolimits       = read.csv('sample1_100k.csv')[2:12]
s3.250k.nolimits       = read.csv('sample1_250k.csv')[2:12]
s4.50k.wlimits         = read.csv('sample2_wlimits_50k.csv')[2:12]
s5.100k.wlimits        = read.csv('sample2_wlimits_100k.csv')[2:12]
s6.250k.wlimits        = read.csv('sample2_wlimits_250k.csv')[2:12]
# SET SEED FOR ENTIRE CODE________________________________________________________________
set.seed(123)
# RANDOMIZE DATA__________________________________________________________________________
s1.50k.nolimits_ran    = s1.50k.nolimits[sample(nrow(s1.50k.nolimits)),]
s2.100k.nolimits_ran   = s2.100k.nolimits[sample(nrow(s2.100k.nolimits)),]
s3.250k.nolimits_ran   = s3.250k.nolimits[sample(nrow(s3.250k.nolimits)),]
s4.50k.wlimits_ran     = s4.50k.wlimits[sample(nrow(s4.50k.wlimits)), ]
s5.100k.wlimits_ran    = s5.100k.wlimits[sample(nrow(s5.100k.wlimits)), ]
s6.250k.wlimits_ran    = s6.250k.wlimits[sample(nrow(s6.250k.wlimits)), ]
## CLEAR NAMESPACE________________________________________________________________________
rm(list = ls())
## IMPORT LIBRARIES_______________________________________________________________________
library(lattice)
library(ggplot2)
library(caret)  # used for parameter tuning
library(glmnet)
library(pls)
library(ISLR)
library(boot)
## CREATE DATASET_________________________________________________________________________
setwd('/home/ccirelli2/Desktop/Repositories/ML_Final_Project_2019/Gotham_Cabs/data')
s1.50k.nolimits        = read.csv('sample1_50k.csv')[2:12]                          #[2:12] drop datetime col.
s2.100k.nolimits       = read.csv('sample1_100k.csv')[2:12]
s3.250k.nolimits       = read.csv('sample1_250k.csv')[2:12]
s4.50k.wlimits         = read.csv('sample2_wlimits_50k.csv')[2:12]
s5.100k.wlimits        = read.csv('sample2_wlimits_100k.csv')[2:12]
s6.250k.wlimits        = read.csv('sample2_wlimits_250k.csv')[2:12]
# SET SEED FOR ENTIRE CODE________________________________________________________________
set.seed(123)
# RANDOMIZE DATA__________________________________________________________________________
s1.50k.nolimits_ran    = s1.50k.nolimits[sample(nrow(s1.50k.nolimits)),]
s2.100k.nolimits_ran   = s2.100k.nolimits[sample(nrow(s2.100k.nolimits)),]
s3.250k.nolimits_ran   = s3.250k.nolimits[sample(nrow(s3.250k.nolimits)),]
s4.50k.wlimits_ran     = s4.50k.wlimits[sample(nrow(s4.50k.wlimits)), ]
s5.100k.wlimits_ran    = s5.100k.wlimits[sample(nrow(s5.100k.wlimits)), ]
s6.250k.wlimits_ran    = s6.250k.wlimits[sample(nrow(s6.250k.wlimits)), ]
## S1 - Factor
s1.50k.nolimits_ran$pickup_x    <- factor(s1.50k.nolimits_ran$pickup_x)
s1.50k.nolimits_ran$pickup_y    <- factor(s1.50k.nolimits_ran$pickup_y)
s1.50k.nolimits_ran$dropoff_x   <- factor(s1.50k.nolimits_ran$dropoff_x)
s1.50k.nolimits_ran$dropoff_y   <- factor(s1.50k.nolimits_ran$dropoff_y)
s1.50k.nolimits_ran$weekday     <- factor(s1.50k.nolimits_ran$weekday)
s1.50k.nolimits_ran$hour_       <- factor(s1.50k.nolimits_ran$hour_)
s1.50k.nolimits_ran$day_        <- factor(s1.50k.nolimits_ran$day_)
s1.50k.nolimits_ran$month_      <- factor(s1.50k.nolimits_ran$month_)
s1.50k.nolimits_ran             <- dummy_cols(s1.50k.nolimits_ran)               # dummy_cols() auto converts all factor cols to dummy_cols.
s1.50k.nolimits_ran             <- dummy_cols(s1.50k.nolimits_ran)               # dummy_cols() auto converts all factor cols to dummy_cols.
library(fastDummies)
s1.50k.nolimits_ran             <- dummy_cols(s1.50k.nolimits_ran)               # dummy_cols() auto converts all factor cols to dummy_cols.
s2.100k.nolimits_ran$pickup_x    <- factor(s2.100k.nolimits_ran$pickup_x)
s2.100k.nolimits_ran$pickup_y    <- factor(s2.100k.nolimits_ran$pickup_y)
s2.100k.nolimits_ran$dropoff_x   <- factor(s2.100k.nolimits_ran$dropoff_x)
s2.100k.nolimits_ran$dropoff_y   <- factor(s2.100k.nolimits_ran$dropoff_y)
s2.100k.nolimits_ran$weekday     <- factor(s2.100k.nolimits_ran$weekday)
s2.100k.nolimits_ran$hour_       <- factor(s2.100k.nolimits_ran$hour_)
s2.100k.nolimits_ran$day_        <- factor(s2.100k.nolimits_ran$day_)
s2.100k.nolimits_ran$month_      <- factor(s2.100k.nolimits_ran$month_)
s2.100k.nolimits_ran             <- dummy_cols(s2.100k.nolimits_ran)              # Create Dummy Columns
s4.50k.wlimits_ran$pickup_x     <- factor(s4.50k.wlimits_ran$pickup_x)
s4.50k.wlimits_ran$pickup_y     <- factor(s4.50k.wlimits_ran$pickup_y)
s4.50k.wlimits_ran$dropoff_x    <- factor(s4.50k.wlimits_ran$dropoff_x)
s4.50k.wlimits_ran$dropoff_y    <- factor(s4.50k.wlimits_ran$dropoff_y)
s4.50k.wlimits_ran$weekday      <- factor(s4.50k.wlimits_ran$weekday)
s4.50k.wlimits_ran$hour_        <- factor(s4.50k.wlimits_ran$hour_)
s4.50k.wlimits_ran$day_         <- factor(s4.50k.wlimits_ran$day_)
s4.50k.wlimits_ran$month_       <- factor(s4.50k.wlimits_ran$month_)
s4.50k.wlimits_ran              <- dummy_cols(s4.50k.wlimits_ran)               # Create Dummy Columns
s5.100k.wlimits_ran$pickup_x    <- factor(s5.100k.wlimits_ran$pickup_x)
s5.100k.wlimits_ran$pickup_y    <- factor(s5.100k.wlimits_ran$pickup_y)
s5.100k.wlimits_ran$dropoff_x   <- factor(s5.100k.wlimits_ran$dropoff_x)
s5.100k.wlimits_ran$dropoff_y   <- factor(s5.100k.wlimits_ran$dropoff_y)
s5.100k.wlimits_ran$weekday     <- factor(s5.100k.wlimits_ran$weekday)
s5.100k.wlimits_ran$hour_       <- factor(s5.100k.wlimits_ran$hour_)
s5.100k.wlimits_ran$day_        <- factor(s5.100k.wlimits_ran$day_)
s5.100k.wlimits_ran$month_      <- factor(s5.100k.wlimits_ran$month_)
s5.100k.wlimits_ran             <- dummy_cols(s5.100k.wlimits_ran)              # Create Dummy Columns
# Calculate Number of Training Observations
train_nrows_50k  = (nrow(s1.50k.nolimits)  * .7)
train_nrows_100k = (nrow(s2.100k.nolimits)   * .7)
# Train
s1.train = s1.50k.nolimits_ran[1:  (nrow(s1.50k.nolimits_ran)  * .7), ]
s2.train = s2.100k.nolimits_ran[1: (nrow(s2.100k.nolimits_ran) * .7), ]
s4.train = s4.50k.wlimits_ran[1:   (nrow(s4.50k.wlimits_ran)  * .7), ]
s5.train = s5.100k.wlimits_ran[1:  (nrow(s5.100k.wlimits_ran)  * .7), ]
# Test
s1.test = s1.50k.nolimits_ran[train_nrows_50k:    nrow(s1.50k.nolimits_ran), ] # Index from training to total
s2.test = s2.100k.nolimits_ran[train_nrows_100k:  nrow(s2.100k.nolimits_ran), ]
s4.test = s4.50k.wlimits_ran[train_nrows_50k:     nrow(s4.50k.wlimits_ran), ]
s5.test = s5.100k.wlimits_ran[train_nrows_100k:   nrow(s5.100k.wlimits_ran), ]
for (i in seq(1,2)){
rse = mlr.poly(s1.train, s1.test, i, 'train')
print(paste('Polynomial =>', i, 'RSE TEST =>', rse))
}
mlr.poly <- function(data_train, data_test, polynomial, objective) {
# Train Model
m1.mlr.poly = lm(duration ~ poly(pickup_x, pickup_y, dropoff_x, dropoff_y, weekday, hour_, day_, distance, month_, speed,
degree = polynomial, raw = T), data = data_train)
# Return Summary Stats
if (objective == 'train'){
m1.summary = summary(m1.mlr.poly)
return(m1.summary)
}
# Generate Prediction
if (objective == 'test'){
m1.mlr.poly.predict    = predict(m1.mlr.poly, data_test)
m1.mlr.poly.rse        = sqrt(sum((s4.test$duration - m1.mlr.poly.predict)^2) / (length(m1.mlr.poly.predict) -2))
return(m1.mlr.poly.rse)
}
}
for (i in seq(1,2)){
rse = mlr.poly(s1.train, s1.test, i, 'train')
print(paste('Polynomial =>', i, 'RSE TEST =>', rse))
}
## CLEAR NAMESPACE________________________________________________________________________
rm(list = ls())
## IMPORT LIBRARIES_______________________________________________________________________
library(lattice)
library(ggplot2)
library(caret)  # used for parameter tuning
library(glmnet)
library(pls)
library(ISLR)
library(boot)
library(fastDummies)
## CREATE DATASET_________________________________________________________________________
setwd('/home/ccirelli2/Desktop/Repositories/ML_Final_Project_2019/Gotham_Cabs/data')
s1.50k.nolimits        = read.csv('sample1_50k.csv')[2:12]                          #[2:12] drop datetime col.
s2.100k.nolimits       = read.csv('sample1_100k.csv')[2:12]
s3.250k.nolimits       = read.csv('sample1_250k.csv')[2:12]
s4.50k.wlimits         = read.csv('sample2_wlimits_50k.csv')[2:12]
s5.100k.wlimits        = read.csv('sample2_wlimits_100k.csv')[2:12]
s6.250k.wlimits        = read.csv('sample2_wlimits_250k.csv')[2:12]
# SET SEED FOR ENTIRE CODE________________________________________________________________
set.seed(123)
# RANDOMIZE DATA__________________________________________________________________________
s1.50k.nolimits_ran    = s1.50k.nolimits[sample(nrow(s1.50k.nolimits)),]
s2.100k.nolimits_ran   = s2.100k.nolimits[sample(nrow(s2.100k.nolimits)),]
s3.250k.nolimits_ran   = s3.250k.nolimits[sample(nrow(s3.250k.nolimits)),]
s4.50k.wlimits_ran     = s4.50k.wlimits[sample(nrow(s4.50k.wlimits)), ]
s5.100k.wlimits_ran    = s5.100k.wlimits[sample(nrow(s5.100k.wlimits)), ]
s6.250k.wlimits_ran    = s6.250k.wlimits[sample(nrow(s6.250k.wlimits)), ]
# Calculate Number of Training Observations
train_nrows_50k  = (nrow(s1.50k.nolimits)  * .7)
train_nrows_100k = (nrow(s2.100k.nolimits)   * .7)
train_nrows_250k = (nrow(s3.250k.nolimits)   * .7)
# Train
s1.train = s1.50k.nolimits_ran[1:  (nrow(s1.50k.nolimits_ran)  * .7), ]
s2.train = s2.100k.nolimits_ran[1: (nrow(s2.100k.nolimits_ran) * .7), ]
s3.train = s3.250k.nolimits_ran[1: (nrow(s2.100k.nolimits_ran) * .7), ]
s4.train = s4.50k.wlimits_ran[1:   (nrow(s4.50k.wlimits_ran)  * .7), ]
s5.train = s5.100k.wlimits_ran[1:  (nrow(s5.100k.wlimits_ran)  * .7), ]
s6.train = s6.250k.wlimits_ran[1:  (nrow(s6.250k.wlimits_ran)  * .7), ]
# Test
s1.test = s1.50k.nolimits_ran[train_nrows_50k:    nrow(s1.50k.nolimits_ran), ] # Index from training to total
s2.test = s2.100k.nolimits_ran[train_nrows_100k:  nrow(s2.100k.nolimits_ran), ]
s3.test = s3.250k.nolimits_ran[train_nrows_250k:  nrow(s3.250k.nolimits_ran), ]
s4.test = s4.50k.wlimits_ran[train_nrows_50k:     nrow(s4.50k.wlimits_ran), ]
s5.test = s5.100k.wlimits_ran[train_nrows_100k:   nrow(s5.100k.wlimits_ran), ]
s6.test = s6.250k.wlimits_ran[train_nrows_250k:   nrow(s6.250k.wlimits_ran), ]
?trainControl()
mlr.poly <- function(dataset, num_polynomials, train_control, result2return) {
'**Model Method      CV w/ 10 folds, find the best feature selection w/ polynomials
dataset             Dataset on which we will train our model.
num_polynomials     The polynomial degree up to which (1-n) we should train our model.
opt_method          Either leapBackward or leapForward
train_control       cv = Cross Validation, number = number of folds and iterations
results             Dataframe with the training error rate and values for the tuning params
bestTune            Dataframe with the final parameters
finalModel          Fit object using the best parameters
'
# Model Setup
'Regress duration on all features w/ n degree of polynomials'
m0 = train(duration ~ poly(pickup_x, pickup_y, dropoff_x, dropoff_y, weekday, hour_, day_, distance, month_, speed,
degree = num_polynomial, raw = T),
data      = dataset,
method    = opt_method,                                   # Step selection
tuneGrid  = data.frame(nvmax = 1 : num_param),            # Number of features to consider in the model
trControl = train_control
)
# Results
if (result2return == 'results'){
print(paste('Results:',m0$results))
return(m0$results)}
# Final Model - # An asterisk specifies that the feature was included in the model
else if (result2return == 'finalModel'){
print(summary(m0$finalModel))
return(summary(m1.backward$finalModel))}
# Get Model w / Best Num Parameters
else if (result2return == 'bestTune'){
print(paste('Best Tune:', m0$bestTune))
return(m0$bestTune)}
}
## CLEAR NAMESPACE________________________________________________________________________
rm(list = ls())
## IMPORT LIBRARIES_______________________________________________________________________
library(lattice)
library(ggplot2)
library(caret)  # used for parameter tuning
library(glmnet)
library(pls)
library(ISLR)
library(boot)
library(fastDummies)
## CREATE DATASET_________________________________________________________________________
setwd('/home/ccirelli2/Desktop/Repositories/ML_Final_Project_2019/Gotham_Cabs/data')
s1.50k.nolimits        = read.csv('sample1_50k.csv')[2:12]                          #[2:12] drop datetime col.
s2.100k.nolimits       = read.csv('sample1_100k.csv')[2:12]
s3.250k.nolimits       = read.csv('sample1_250k.csv')[2:12]
s4.50k.wlimits         = read.csv('sample2_wlimits_50k.csv')[2:12]
s5.100k.wlimits        = read.csv('sample2_wlimits_100k.csv')[2:12]
s6.250k.wlimits        = read.csv('sample2_wlimits_250k.csv')[2:12]
# SET SEED FOR ENTIRE CODE________________________________________________________________
set.seed(123)
# RANDOMIZE DATA__________________________________________________________________________
s1.50k.nolimits_ran    = s1.50k.nolimits[sample(nrow(s1.50k.nolimits)),]
s2.100k.nolimits_ran   = s2.100k.nolimits[sample(nrow(s2.100k.nolimits)),]
s3.250k.nolimits_ran   = s3.250k.nolimits[sample(nrow(s3.250k.nolimits)),]
s4.50k.wlimits_ran     = s4.50k.wlimits[sample(nrow(s4.50k.wlimits)), ]
s5.100k.wlimits_ran    = s5.100k.wlimits[sample(nrow(s5.100k.wlimits)), ]
s6.250k.wlimits_ran    = s6.250k.wlimits[sample(nrow(s6.250k.wlimits)), ]
# Calculate Number of Training Observations
train_nrows_50k  = (nrow(s1.50k.nolimits)  * .7)
train_nrows_100k = (nrow(s2.100k.nolimits)   * .7)
train_nrows_250k = (nrow(s3.250k.nolimits)   * .7)
# Train
s1.train = s1.50k.nolimits_ran[1:  (nrow(s1.50k.nolimits_ran)  * .7), ]
s2.train = s2.100k.nolimits_ran[1: (nrow(s2.100k.nolimits_ran) * .7), ]
s3.train = s3.250k.nolimits_ran[1: (nrow(s2.100k.nolimits_ran) * .7), ]
s4.train = s4.50k.wlimits_ran[1:   (nrow(s4.50k.wlimits_ran)  * .7), ]
s5.train = s5.100k.wlimits_ran[1:  (nrow(s5.100k.wlimits_ran)  * .7), ]
s6.train = s6.250k.wlimits_ran[1:  (nrow(s6.250k.wlimits_ran)  * .7), ]
# Test
s1.test = s1.50k.nolimits_ran[train_nrows_50k:    nrow(s1.50k.nolimits_ran), ] # Index from training to total
s2.test = s2.100k.nolimits_ran[train_nrows_100k:  nrow(s2.100k.nolimits_ran), ]
s3.test = s3.250k.nolimits_ran[train_nrows_250k:  nrow(s3.250k.nolimits_ran), ]
s4.test = s4.50k.wlimits_ran[train_nrows_50k:     nrow(s4.50k.wlimits_ran), ]
s5.test = s5.100k.wlimits_ran[train_nrows_100k:   nrow(s5.100k.wlimits_ran), ]
s6.test = s6.250k.wlimits_ran[train_nrows_250k:   nrow(s6.250k.wlimits_ran), ]
mlr.poly <- function(dataset, num_polynomials, train_control, result2return) {
'**Model Method      CV w/ 10 folds, find the best feature selection w/ polynomials
dataset             Dataset on which we will train our model.
num_polynomials     The polynomial degree up to which (1-n) we should train our model.
opt_method          Either leapBackward or leapForward
train_control       cv = Cross Validation, number = number of folds and iterations
results             Dataframe with the training error rate and values for the tuning params
bestTune            Dataframe with the final parameters
finalModel          Fit object using the best parameters
'
# Model Setup
'Regress duration on all features w/ n degree of polynomials'
m0 = train(duration ~ poly(pickup_x, pickup_y, dropoff_x, dropoff_y, weekday, hour_, day_, distance, month_, speed,
degree = num_polynomial, raw = T),
data      = dataset,
method    = opt_method,                                   # Step selection
tuneGrid  = data.frame(nvmax = 1 : num_param),            # Number of features to consider in the model
trControl = train_control
)
# Results
if (result2return == 'results'){
print(paste('Results:',m0$results))
return(m0$results)}
# Final Model - # An asterisk specifies that the feature was included in the model
else if (result2return == 'finalModel'){
print(summary(m0$finalModel))
return(summary(m1.backward$finalModel))}
# Get Model w / Best Num Parameters
else if (result2return == 'bestTune'){
print(paste('Best Tune:', m0$bestTune))
return(m0$bestTune)}
}
# Create Training Method - Method = Cross Validation, Folds = 10
train.control = trainControl(method = 'cv', number = 10)
for (i in seq(1,2)){
rse = mlr.poly(s4.50k.wlimits_ran, i, train_control, 'results')
print(paste('Polynomial =>', i, 'RSE TEST =>', rse))
}
mlr.poly <- function(dataset, num_polynomials, train_control, result2return) {
'**Model Method      CV w/ 10 folds, find the best feature selection w/ polynomials
dataset             Dataset on which we will train our model.
num_polynomials     The polynomial degree up to which (1-n) we should train our model.
opt_method          Either leapBackward or leapForward
train_control       cv = Cross Validation, number = number of folds and iterations
results             Dataframe with the training error rate and values for the tuning params
bestTune            Dataframe with the final parameters
finalModel          Fit object using the best parameters
'
# Model Setup
'Regress duration on all features w/ n degree of polynomials'
m0 = train(duration ~ poly(pickup_x, pickup_y, dropoff_x, dropoff_y, weekday, hour_, day_, distance, month_, speed,
degree = num_polynomials, raw = T),
data      = dataset,
method    = opt_method,                                   # Step selection
tuneGrid  = data.frame(nvmax = 1 : num_param),            # Number of features to consider in the model
trControl = train_control
)
# Results
if (result2return == 'results'){
print(paste('Results:',m0$results))
return(m0$results)}
# Final Model - # An asterisk specifies that the feature was included in the model
else if (result2return == 'finalModel'){
print(summary(m0$finalModel))
return(summary(m1.backward$finalModel))}
# Get Model w / Best Num Parameters
else if (result2return == 'bestTune'){
print(paste('Best Tune:', m0$bestTune))
return(m0$bestTune)}
}
for (i in seq(1,2)){
rse = mlr.poly(s4.50k.wlimits_ran, i, train_control, 'results')
print(paste('Polynomial =>', i, 'RSE TEST =>', rse))
}
for (i in seq(1,2)){
rse = mlr.poly(s4.50k.wlimits_ran, i, train_control, 'leapForward', 'results')
print(paste('Polynomial =>', i, 'RSE TEST =>', rse))
}
mlr.poly <- function(dataset, num_polynomials, train_control, opt_method, result2return) {
'**Model Method      CV w/ 10 folds, find the best feature selection w/ polynomials
dataset             Dataset on which we will train our model.
num_polynomials     The polynomial degree up to which (1-n) we should train our model.
opt_method          Either leapBackward or leapForward
train_control       cv = Cross Validation, number = number of folds and iterations
results             Dataframe with the training error rate and values for the tuning params
bestTune            Dataframe with the final parameters
finalModel          Fit object using the best parameters
'
# Model Setup
'Regress duration on all features w/ n degree of polynomials'
m0 = train(duration ~ poly(pickup_x, pickup_y, dropoff_x, dropoff_y, weekday, hour_, day_, distance, month_, speed,
degree = num_polynomials, raw = T),
data      = dataset,
method    = opt_method,                                   # Step selection
tuneGrid  = data.frame(nvmax = 1 : num_param),            # Number of features to consider in the model
trControl = train_control
)
# Results
if (result2return == 'results'){
print(paste('Results:',m0$results))
return(m0$results)}
# Final Model - # An asterisk specifies that the feature was included in the model
else if (result2return == 'finalModel'){
print(summary(m0$finalModel))
return(summary(m1.backward$finalModel))}
# Get Model w / Best Num Parameters
else if (result2return == 'bestTune'){
print(paste('Best Tune:', m0$bestTune))
return(m0$bestTune)}
}
for (i in seq(1,2)){
rse = mlr.poly(dataset = s4.50k.wlimits_ran, num_polynomials = i, train_control = train.control,
opt_method = 'leapForward', result2return = 'results')
print(paste('Polynomial =>', i, 'RSE TEST =>', rse))
}
for (i in seq(1,2)){
rse = mlr.poly(dataset = s4.50k.wlimits_ran, num_polynomials = i, num_param = 3, train_control = train.control,
opt_method = 'leapForward', result2return = 'results')
print(paste('Polynomial =>', i, 'RSE TEST =>', rse))
}
mlr.poly <- function(dataset, num_polynomials, num_param, train_control, opt_method, result2return) {
'**Model Method      CV w/ 10 folds, find the best feature selection w/ polynomials
dataset             Dataset on which we will train our model.
num_polynomials     The polynomial degree up to which (1-n) we should train our model.
num_param           The number of parameters to include in the stepwise selection
opt_method          Either leapBackward or leapForward
train_control       cv = Cross Validation, number = number of folds and iterations
results             Dataframe with the training error rate and values for the tuning params
bestTune            Dataframe with the final parameters
finalModel          Fit object using the best parameters
'
# Model Setup
'Regress duration on all features w/ n degree of polynomials'
m0 = train(duration ~ poly(pickup_x, pickup_y, dropoff_x, dropoff_y, weekday, hour_, day_, distance, month_, speed,
degree = num_polynomials, raw = T),
data      = dataset,
method    = opt_method,                                   # Step selection
tuneGrid  = data.frame(nvmax = 1 : num_param),            # Number of features to consider in the model
trControl = train_control
)
# Results
if (result2return == 'results'){
print(paste('Results:',m0$results))
return(m0$results)}
# Final Model - # An asterisk specifies that the feature was included in the model
else if (result2return == 'finalModel'){
print(summary(m0$finalModel))
return(summary(m1.backward$finalModel))}
# Get Model w / Best Num Parameters
else if (result2return == 'bestTune'){
print(paste('Best Tune:', m0$bestTune))
return(m0$bestTune)}
}
for (i in seq(1,2)){
rse = mlr.poly(dataset = s4.50k.wlimits_ran, num_polynomials = i, num_param = 3, train_control = train.control,
opt_method = 'leapForward', result2return = 'results')
print(paste('Polynomial =>', i, 'RSE TEST =>', rse))
}
mlr.poly <- function(dataset, num_polynomials, num_param, train_control, opt_method, result2return) {
'**Model Method      CV w/ 10 folds, find the best feature selection w/ polynomials
dataset             Dataset on which we will train our model.
num_polynomials     The polynomial degree up to which (1-n) we should train our model.
num_param           The number of parameters to include in the stepwise selection
opt_method          Either leapBackward or leapForward
train_control       cv = Cross Validation, number = number of folds and iterations
results             Dataframe with the training error rate and values for the tuning params
bestTune            Dataframe with the final parameters
finalModel          Fit object using the best parameters
'
# Model Setup
'Regress duration on all features w/ n degree of polynomials'
f <- bquote(duration ~ poly(duration, .(i)))
m0 = train(as.formula(f),
data      = dataset,
method    = opt_method,                                   # Step selection
tuneGrid  = data.frame(nvmax = 1 : num_param),            # Number of features to consider in the model
trControl = train_control)
'
m0 = train(duration ~ poly(pickup_x, pickup_y, dropoff_x, dropoff_y, weekday, hour_, day_, distance, month_, speed,
degree = num_polynomials, raw = T),
data      = dataset,
method    = opt_method,                                   # Step selection
tuneGrid  = data.frame(nvmax = 1 : num_param),            # Number of features to consider in the model
trControl = train_control
)'
# Results
if (result2return == 'results'){
print(paste('Results:',m0$results))
return(m0$results)}
# Final Model - # An asterisk specifies that the feature was included in the model
else if (result2return == 'finalModel'){
print(summary(m0$finalModel))
return(summary(m1.backward$finalModel))}
# Get Model w / Best Num Parameters
else if (result2return == 'bestTune'){
print(paste('Best Tune:', m0$bestTune))
return(m0$bestTune)}
}
for (i in seq(1,2)){
rse = mlr.poly(dataset = s4.50k.wlimits_ran, num_polynomials = i, num_param = 3, train_control = train.control,
opt_method = 'leapForward', result2return = 'results')
print(paste('Polynomial =>', i, 'RSE TEST =>', rse))
}
