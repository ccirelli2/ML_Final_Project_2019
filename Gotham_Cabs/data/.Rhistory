s5.100k.wlimits        = read.csv('sample2_wlimits_100k.csv')[2:12]
s6.250k.wlimits        = read.csv('sample2_wlimits_250k.csv')[2:12]
# SET SEED FOR ENTIRE CODE________________________________________________________________
set.seed(123)
s1.50k.nolimits_ran    = s1.50k.nolimits[sample(nrow(s1.50k.nolimits)),]
s2.100k.nolimits_ran   = s2.100k.nolimits[sample(nrow(s2.100k.nolimits)),]
s3.250k.nolimits_ran   = s3.250k.nolimits[sample(nrow(s3.250k.nolimits)),]
s4.50k.wlimits_ran     = s4.50k.wlimits[sample(nrow(s4.50k.wlimits)), ]
s5.100k.wlimits_ran    = s5.100k.wlimits[sample(nrow(s5.100k.wlimits)), ]
s6.250k.wlimits_ran    = s6.250k.wlimits[sample(nrow(s6.250k.wlimits)), ]
# Separate Target & Feature Values
s1_y = s1.50k.nolimits_ran$duration
s1_x = as.matrix(s1.50k.nolimits_ran[,2:11])
s2_y = s2.100k.nolimits_ran$duration
s2_x = as.matrix(s2.100k.nolimits_ran[,2:11])
s3_y = s3.250k.nolimits_ran$duration
s3_x = as.matrix(s3.250k.nolimits_ran[,2:11])
s4_y = s4.50k.wlimits_ran$duration
s4_x = as.matrix(s4.50k.wlimits_ran[,2:11])
s5_y = s5.100k.wlimits_ran$duration
s5_x = as.matrix(s5.100k.wlimits_ran[,2:11])
s6_y = s6.250k.wlimits_ran$duration
s6_x = as.matrix(s6.250k.wlimits_ran[,2:11])
# Generate Grid Possible Values Lambda
grid = 10^seq(from = 10, to = -2, length = 100)                 #length = desired length of sequence
## CLEAR NAMESPACE________________________________________________________________________
rm(list = ls())
## IMPORT LIBRARIES_______________________________________________________________________
library(lattice)
library(ggplot2)
library(caret)  # used for parameter tuning
## CREATE DATASET_________________________________________________________________________
setwd('/home/ccirelli2/Desktop/Repositories/ML_Final_Project_2019/Gotham_Cabs/data')
s1.50k.nolimits        = read.csv('sample1_50k.csv')[2:12]                          #[2:12] drop datetime col.
s2.100k.nolimits       = read.csv('sample1_100k.csv')[2:12]
s3.250k.nolimits       = read.csv('sample1_250k.csv')[2:12]
s4.50k.wlimits         = read.csv('sample2_wlimits_50k.csv')[2:12]
s5.100k.wlimits        = read.csv('sample2_wlimits_100k.csv')[2:12]
s6.250k.wlimits        = read.csv('sample2_wlimits_250k.csv')[2:12]
# SET SEED FOR ENTIRE CODE________________________________________________________________
set.seed(123)
# RANDOMIZE DATA__________________________________________________________________________
s1.50k.nolimits_ran    = s1.50k.nolimits[sample(nrow(s1.50k.nolimits)),]
s2.100k.nolimits_ran   = s2.100k.nolimits[sample(nrow(s2.100k.nolimits)),]
s3.250k.nolimits_ran   = s3.250k.nolimits[sample(nrow(s3.250k.nolimits)),]
s4.50k.wlimits_ran     = s4.50k.wlimits[sample(nrow(s4.50k.wlimits)), ]
s5.100k.wlimits_ran    = s5.100k.wlimits[sample(nrow(s5.100k.wlimits)), ]
s6.250k.wlimits_ran    = s6.250k.wlimits[sample(nrow(s6.250k.wlimits)), ]
# TRAIN / TEST SPLIT______________________________________________________________________
# Calculate Number of Training Observations
train_nrows_50k  = (nrow(s1.50k.nolimits)  * .7)
train_nrows_100k = (nrow(s2.100k.nolimits)   * .7)
train_nrows_250k = (nrow(s3.250k.nolimits)   * .7)
# Train
s1.train = s1.50k.nolimits_ran[1:   train_nrows_50k, ]
s2.train = s2.100k.nolimits_ran[1:  train_nrows_100k, ]
s3.train = s3.250k.nolimits_ran[1:  train_nrows_250k, ]
s4.train = s4.50k.wlimits_ran[1:    train_nrows_50k, ]
s5.train = s5.100k.wlimits_ran[1:   train_nrows_100k, ]
s6.train = s6.250k.wlimits_ran[1:   train_nrows_250k, ]
# Test
s1.test = s1.50k.nolimits_ran[train_nrows_50k:    nrow(s1.50k.nolimits_ran), ] # Index from training to total
s2.test = s2.100k.nolimits_ran[train_nrows_100k:  nrow(s2.100k.nolimits_ran), ]
s3.test = s3.250k.nolimits_ran[train_nrows_250k:  nrow(s3.250k.nolimits_ran), ]
s4.test = s4.50k.wlimits_ran[train_nrows_50k:     nrow(s4.50k.wlimits_ran), ]
s5.test = s5.100k.wlimits_ran[train_nrows_100k:   nrow(s5.100k.wlimits_ran), ]
s6.test = s6.250k.wlimits_ran[train_nrows_250k:   nrow(s6.250k.wlimits_ran), ]
mlr.poly <- function(data_train, data_test, polynomial, objective) {
# Train Model
m1.mlr.poly = lm(duration ~ poly(pickup_x, pickup_y, dropoff_x, dropoff_y,
weekday, hour_,  day_,distance,
month_, speed,
degree = polynomial, raw = T), data = data_train)
# Return Summary Stats
if (objective == 'train'){
m1.summary               = summary(m1.mlr.poly)
train.rse                = sqrt((sum(m1.summary$residuals^2)) / length(m1.summary$residuals) -2)
return(train.rse)
}
# Generate Prediction
if (objective == 'test'){
m1.mlr.poly.predict    = predict(m1.mlr.poly, data_test)
test.rse               = sqrt(sum((data_test$duration - m1.mlr.poly.predict)^2) / (length(m1.mlr.poly.predict) -2))
return(test.rse)
}
}
index.rse = c()
list.train.rse = c()
list.test.rse  = c()
index_count = 1
for (i in seq(1, 3.5, 0.1)){
index.rse[index_count] = i
print(paste('creating test set for poly =>', i))
train.rse = mlr.poly(s1.train, s1.test, i, 'train')
list.train.rse[index_count] = train.rse
print(paste('creating train set for poly =>', i))
test.rse = mlr.poly(s1.train, s1.test, i, 'test')
list.test.rse[index_count] = test.rse
index_count = index_count + 1
}
df = data.frame(row.names = index.rse)
df$index.rse = index.rse
df$train.rse = list.train.rse
df$test.rse = list.test.rse
df$test.rse
index.rse = c()
list.train.rse = c()
list.test.rse  = c()
index_count = 1
for (i in seq(1, 3.5, 0.1)){
index.rse[index_count] = i
print(paste('creating test set for poly =>', i))
train.rse = mlr.poly(s4.train, s4.test, i, 'train')
list.train.rse[index_count] = round(train.rse, 2)
print(paste('creating train set for poly =>', i))
test.rse = mlr.poly(s1.train, s1.test, i, 'test')
print(paste('Test RSE', round(train.rse), 2))
list.test.rse[index_count] = round(test.rse,2)
index_count = index_count + 1
}
df = data.frame(row.names = index.rse)
df$index.rse = index.rse
df$train.rse = list.train.rse
df$test.rse = list.test.rse
df$test.rse
for (i in seq(1, 3.5, 0.1)){
index.rse[index_count] = i
print(paste('creating test set for poly =>', i))
train.rse = mlr.poly(s4.train, s4.test, i, 'train')
list.train.rse[index_count] = round(train.rse, 2)
print(paste('creating train set for poly =>', i))
test.rse = mlr.poly(s1.train, s1.test, i, 'test')
print(paste('Test RSE', round(test.rse), 2))
list.test.rse[index_count] = round(test.rse,2)
index_count = index_count + 1
}
rm(list = ls())
## IMPORT LIBRARIES_______________________________________________________________________
library(lattice)
library(ggplot2)
library(caret)  # used for parameter tuning
## CREATE DATASET_________________________________________________________________________
setwd('/home/ccirelli2/Desktop/Repositories/ML_Final_Project_2019/Gotham_Cabs/data')
s1.50k.nolimits        = read.csv('sample1_50k.csv')[2:12]                          #[2:12] drop datetime col.
s2.100k.nolimits       = read.csv('sample1_100k.csv')[2:12]
s3.250k.nolimits       = read.csv('sample1_250k.csv')[2:12]
s4.50k.wlimits         = read.csv('sample2_wlimits_50k.csv')[2:12]
s5.100k.wlimits        = read.csv('sample2_wlimits_100k.csv')[2:12]
s6.250k.wlimits        = read.csv('sample2_wlimits_250k.csv')[2:12]
# SET SEED FOR ENTIRE CODE________________________________________________________________
set.seed(123)
# RANDOMIZE DATA__________________________________________________________________________
s1.50k.nolimits_ran    = s1.50k.nolimits[sample(nrow(s1.50k.nolimits)),]
s2.100k.nolimits_ran   = s2.100k.nolimits[sample(nrow(s2.100k.nolimits)),]
s3.250k.nolimits_ran   = s3.250k.nolimits[sample(nrow(s3.250k.nolimits)),]
s4.50k.wlimits_ran     = s4.50k.wlimits[sample(nrow(s4.50k.wlimits)), ]
s5.100k.wlimits_ran    = s5.100k.wlimits[sample(nrow(s5.100k.wlimits)), ]
s6.250k.wlimits_ran    = s6.250k.wlimits[sample(nrow(s6.250k.wlimits)), ]
# TRAIN / TEST SPLIT______________________________________________________________________
# Calculate Number of Training Observations
train_nrows_50k  = (nrow(s1.50k.nolimits)  * .7)
train_nrows_100k = (nrow(s2.100k.nolimits)   * .7)
train_nrows_250k = (nrow(s3.250k.nolimits)   * .7)
# Train
s1.train = s1.50k.nolimits_ran[1:   train_nrows_50k, ]
s2.train = s2.100k.nolimits_ran[1:  train_nrows_100k, ]
s3.train = s3.250k.nolimits_ran[1:  train_nrows_250k, ]
s4.train = s4.50k.wlimits_ran[1:    train_nrows_50k, ]
s5.train = s5.100k.wlimits_ran[1:   train_nrows_100k, ]
s6.train = s6.250k.wlimits_ran[1:   train_nrows_250k, ]
# Test
s1.test = s1.50k.nolimits_ran[train_nrows_50k:    nrow(s1.50k.nolimits_ran), ] # Index from training to total
s2.test = s2.100k.nolimits_ran[train_nrows_100k:  nrow(s2.100k.nolimits_ran), ]
s3.test = s3.250k.nolimits_ran[train_nrows_250k:  nrow(s3.250k.nolimits_ran), ]
s4.test = s4.50k.wlimits_ran[train_nrows_50k:     nrow(s4.50k.wlimits_ran), ]
s5.test = s5.100k.wlimits_ran[train_nrows_100k:   nrow(s5.100k.wlimits_ran), ]
s6.test = s6.250k.wlimits_ran[train_nrows_250k:   nrow(s6.250k.wlimits_ran), ]
mlr.poly <- function(data_train, data_test, polynomial, objective) {
# Train Model
m1.mlr.poly = lm(duration ~ poly(pickup_x, pickup_y, dropoff_x, dropoff_y,
weekday, hour_,  day_,distance,
month_, speed,
degree = polynomial, raw = T), data = data_train)
# Return Summary Stats
if (objective == 'train'){
m1.summary               = summary(m1.mlr.poly)
train.rse                = sqrt((sum(m1.summary$residuals^2)) / length(m1.summary$residuals) -2)
return(train.rse)
}
# Generate Prediction
if (objective == 'test'){
m1.mlr.poly.predict    = predict(m1.mlr.poly, data_test)
test.rse               = sqrt(sum((data_test$duration - m1.mlr.poly.predict)^2) / (length(m1.mlr.poly.predict) -2))
return(test.rse)
}
}
for (i in seq(1, 3.5, 0.1)){
index.rse[index_count] = i
print(paste('creating test set for poly =>', i))
train.rse = mlr.poly(s6.train, s6.test, i, 'train')
list.train.rse[index_count] = round(train.rse, 2)
print(paste('creating train set for poly =>', i))
test.rse = mlr.poly(s1.train, s1.test, i, 'test')
print(paste('Test RSE', round(test.rse), 2))
list.test.rse[index_count] = round(test.rse,2)
index_count = index_count + 1
}
# Create Lists to Capture Values & A DataFrame to House the Columns
index.rse = c()
list.train.rse = c()
list.test.rse  = c()
index_count = 1
# Iterate Over a Sequence of Values For The Polynomials
for (i in seq(1, 3.5, 0.1)){
index.rse[index_count] = i
print(paste('creating test set for poly =>', i))
train.rse = mlr.poly(s6.train, s6.test, i, 'train')
list.train.rse[index_count] = round(train.rse, 2)
print(paste('creating train set for poly =>', i))
test.rse = mlr.poly(s1.train, s1.test, i, 'test')
print(paste('Test RSE', round(test.rse), 2))
list.test.rse[index_count] = round(test.rse,2)
index_count = index_count + 1
}
round(.111111, 2)
list.test.rse
df = data.frame(row.names = index.rse)
df$index.rse = index.rse
df$train.rse = list.train.rse
df$test.rse = list.test.rse
df
# Create Lists to Capture Values & A DataFrame to House the Columns
index.rse = c()
list.train.rse = c()
list.test.rse  = c()
index_count = 1
# Iterate Over a Sequence of Values For The Polynomials
for (i in seq(1, 3.5, 0.1)){
index.rse[index_count] = i
print(paste('creating test set for poly =>', i))
train.rse = mlr.poly(s6.train, s6.test, i, 'train')
list.train.rse[index_count] = round(train.rse, 2)
print(paste('creating train set for poly =>', i))
test.rse = mlr.poly(s1.train, s1.test, i, 'test')
print(paste('Test RSE', round(test.rse), 2))
list.test.rse[index_count] = round(test.rse,2)
index_count = index_count + 1
}
# Create DataFrame
df = data.frame(row.names = index.rse)
df$index.rse = index.rse
df$train.rse = list.train.rse
df$test.rse = list.test.rse
# Generate a Plot for Train & Test Points
p = ggplot() +
geom_line(data = df, aes(x = df$index.rse, y = df$train.rse, color = 'Train RSE')) +
geom_line(data = df, aes(x = df$index.rse, y = df$test.rse, color = 'Test RSE')) +
xlab('Number of Polynomials') +
ylab('RSE')
print(p+ ggtitle('Plot Test & Training Polynomials'))
df$test.rse
df
# Clear Namespace
rm(list = ls())
model.names = c('SLR.RSE', 'MLR.DUMMIES.RSE', 'MLR.STEPWISE.RMSE', 'MLR.L1L2.RSE', 'MLR.RSE', 'MLR.POLY.RSE')
model.rse   = c(343.66, 246.85, 245.8, 241.04, 241.041, 239.39)
# Create DataFrame
df = data.frame(row.names = model.names)
df$RSE = model.rse
ggplot(df, aes(y = df$RSE, x = reorder(model.names, -df$RSE), fill = model.names)) +
geom_p(stat = 'identity') +
ggtitle('Multilinear Regression - RSE Over 6 Datasets')  +
scale_y_continuous(breaks = pretty(df$RSE, n = 5))
ggplot(df, aes(y = df$RSE, x = reorder(model.names, -df$RSE), fill = model.names)) +
geom_bar(stat = 'identity') +
ggtitle('Multilinear Regression - RSE Over 6 Datasets')  +
scale_y_continuous(breaks = pretty(df$RSE, n = 5))
rm(list = ls())
## IMPORT LIBRARIES_______________________________________________________________________
library(lattice)
library(ggplot2)
library(caret)  # used for parameter tuning
library(glmnet)
library(pls)
library(ISLR)
library(boot)
library(fastDummies)
## CREATE DATASET_________________________________________________________________________
setwd('/home/ccirelli2/Desktop/Repositories/ML_Final_Project_2019/Gotham_Cabs/data')
s1.50k.nolimits        = read.csv('sample1_50k.csv')[2:12]                          #[2:12] drop datetime col.
s2.100k.nolimits       = read.csv('sample1_100k.csv')[2:12]
s3.250k.nolimits       = read.csv('sample1_250k.csv')[2:12]
s4.50k.wlimits         = read.csv('sample2_wlimits_50k.csv')[2:12]
s5.100k.wlimits        = read.csv('sample2_wlimits_100k.csv')[2:12]
s6.250k.wlimits        = read.csv('sample2_wlimits_250k.csv')[2:12]
# SET SEED FOR ENTIRE CODE________________________________________________________________
set.seed(123)
# RANDOMIZE DATA__________________________________________________________________________
s1.50k.nolimits_ran    = s1.50k.nolimits[sample(nrow(s1.50k.nolimits)),]
s2.100k.nolimits_ran   = s2.100k.nolimits[sample(nrow(s2.100k.nolimits)),]
s3.250k.nolimits_ran   = s3.250k.nolimits[sample(nrow(s3.250k.nolimits)),]
s4.50k.wlimits_ran     = s4.50k.wlimits[sample(nrow(s4.50k.wlimits)), ]
s5.100k.wlimits_ran    = s5.100k.wlimits[sample(nrow(s5.100k.wlimits)), ]
s6.250k.wlimits_ran    = s6.250k.wlimits[sample(nrow(s6.250k.wlimits)), ]
# Separate Target & Feature Values
s1_y = s1.50k.nolimits_ran$duration
s1_x = as.matrix(s1.50k.nolimits_ran[,2:11])
s2_y = s2.100k.nolimits_ran$duration
s2_x = as.matrix(s2.100k.nolimits_ran[,2:11])
s3_y = s3.250k.nolimits_ran$duration
s3_x = as.matrix(s3.250k.nolimits_ran[,2:11])
s4_y = s4.50k.wlimits_ran$duration
s4_x = as.matrix(s4.50k.wlimits_ran[,2:11])
s5_y = s5.100k.wlimits_ran$duration
s5_x = as.matrix(s5.100k.wlimits_ran[,2:11])
s6_y = s6.250k.wlimits_ran$duration
s6_x = as.matrix(s6.250k.wlimits_ran[,2:11])
# Generate Grid Possible Values Lambda
grid = 10^seq(from = 10, to = -2, length = 100)                 #length = desired length of sequence
# M1:   TRAIN RIDGE & LASSO MODELS - COMPARE LAMBDAS__________________________________________
# Train Model
m1.ridge <- glmnet(s1_x, s1_y, alpha = 0, lambda = grid, standardize = TRUE)
x <- model.matrix(duration ~ poly(pickup_x, pickup_y, dropoff_x, dropoff_y, weekday, hour_,  day_,distance,
month_, speed, degree = 2.0, raw = T), data = s4.50k.wlimits_ran)
m1.ridge <- glmnet(x, s1_y, alpha = 0, lambda = grid, standardize = TRUE)
m1.ridge
# Define Lists to Capture Output
rse.test = c()
list.lambda    = c()
X.datasets     = list(s1_x, s4_x, s5_x, s6_x)
Y.datasets     = list(s1_y, s4_y, s5_y, s6_y)
names.datasets = c('50knl','50kwl', '100kwl', '250kwl')
for (i in seq(1,4)){
# Create Data Objects
print('Creating Datasets')
X = X.datasets[[i]]
Y = Y.datasets[[i]]
# Use Model Matrix To Add Polynomials For Features
X <- model.matrix(duration ~ poly(pickup_x, pickup_y, dropoff_x, dropoff_y, weekday, hour_,  day_,distance,
month_, speed, degree = 2.0, raw = T), data = X)
# Train Model Using CV
print('Training CV Model')
m_cv = cv.glmnet(X, Y, alpha = 0, lambda = grid, standardize = TRUE, nfolds = 10)
# Get Best Lambda
cv_lambda = m_cv$lambda.min
# Fit Model w/ Best Lambda
print('Fit Model w/ Best Lambda')
m_optimal <- glmnet(X, Y, alpha = 0, lambda = cv_lambda, standardize = TRUE)
# Generate Prediction
print('Generate Prediction')
y_hat_cv <- predict(m_optimal, X)
# Calculate RSE
print('Calculate RSE')
model_cv_rse = sqrt(sum((Y - y_hat_cv)^2) / (length(Y) - 2))
print(paste('Model ', i, 'RSE =>', model_cv_rse))
# Append RSE Values To List
rse.test[i] = round(model_cv_rse,0)
print(paste('Iteration', i, 'completed'))
}
X = X.datasets[[i]]
Y = Y.datasets[[i]]
# Use Model Matrix To Add Polynomials For Features
X <- model.matrix(duration ~ poly(pickup_x, pickup_y, dropoff_x, dropoff_y, weekday, hour_,  day_,distance,
month_, speed, degree = 2.0, raw = T), data = X)
x <- model.matrix(duration ~ poly(pickup_x, pickup_y, dropoff_x, dropoff_y, weekday, hour_,  day_,distance,
month_, speed, degree = 2.0, raw = T), data = s4.50k.wlimits_ran)
m_cv = cv.glmnet(x, y, alpha = 0, lambda = grid, standardize = TRUE, nfolds = 10)
y = s4.50k.wlimits_ran$duration
m_cv = cv.glmnet(x, y, alpha = 0, lambda = grid, standardize = TRUE, nfolds = 10)
cv_lambda = m_cv$lambda.min
m_optimal <- glmnet(X, Y, alpha = 0, lambda = cv_lambda, standardize = TRUE)
m_optimal <- glmnet(x, y, alpha = 0, lambda = cv_lambda, standardize = TRUE)
y_hat_cv <- predict(m_optimal, y)
y_hat_cv <- predict(m_optimal, x)
model_cv_rse = sqrt(sum((Y - y_hat_cv)^2) / (length(Y) - 2))
model_cv_rse
m_cv = cv.glmnet(x, y, alpha = 1, lambda = grid, standardize = TRUE, nfolds = 10)
cv_lambda = m_cv$lambda.min
m_optimal <- glmnet(x, y, alpha = 1, lambda = cv_lambda, standardize = TRUE)
y_hat_cv <- predict(m_optimal, x)
model_cv_rse = sqrt(sum((Y - y_hat_cv)^2) / (length(Y) - 2))
model_cv_rse
x <- model.matrix(duration ~ poly(pickup_x, pickup_y, dropoff_x, dropoff_y, weekday, hour_,  day_,distance,
month_, speed, degree = 1.5, raw = T), data = s4.50k.wlimits_ran)
y = s4.50k.wlimits_ran$duration
m_cv = cv.glmnet(x, y, alpha = 0, lambda = grid, standardize = TRUE, nfolds = 10)
cv_lambda = m_cv$lambda.min
m_optimal <- glmnet(x, y, alpha = 0, lambda = cv_lambda, standardize = TRUE)
y_hat_cv <- predict(m_optimal, x)
model_cv_rse = sqrt(sum((Y - y_hat_cv)^2) / (length(Y) - 2))
model_cv_rse
x <- model.matrix(duration ~ poly(pickup_x, pickup_y, dropoff_x, dropoff_y, weekday, hour_,  day_,distance,
month_, speed, degree = 1.2, raw = T), data = s6.250k.wlimits_ran)
y = s6.250k.wlimits_ran$duration
m_cv = cv.glmnet(x, y, alpha = 0, lambda = grid, standardize = TRUE, nfolds = 10)
cv_lambda = m_cv$lambda.min
m_optimal <- glmnet(x, y, alpha = 0, lambda = cv_lambda, standardize = TRUE)
y_hat_cv <- predict(m_optimal, x)
model_cv_rse = sqrt(sum((Y - y_hat_cv)^2) / (length(Y) - 2))
model_cv_rse
x <- model.matrix(duration ~ poly(pickup_x, pickup_y, dropoff_x, dropoff_y, weekday, hour_,  day_,distance,
month_, speed, degree = 2, raw = T), data = s6.250k.wlimits_ran)
y = s6.250k.wlimits_ran$duration
m_cv = cv.glmnet(x, y, alpha = 0, lambda = grid, standardize = TRUE, nfolds = 10)
cv_lambda = m_cv$lambda.min
m_optimal <- glmnet(x, y, alpha = 0, lambda = cv_lambda, standardize = TRUE)
y_hat_cv <- predict(m_optimal, x)
model_cv_rse = sqrt(sum((Y - y_hat_cv)^2) / (length(Y) - 2))
model_cv_rse
x <- model.matrix(duration ~ poly(pickup_x, pickup_y, dropoff_x, dropoff_y, weekday, hour_,  day_,distance,
month_, speed, degree = 1.5, raw = T), data = s6.250k.wlimits_ran)
y = s6.250k.wlimits_ran$duration
m_cv = cv.glmnet(x, y, alpha = 0, lambda = grid, standardize = TRUE, nfolds = 10)
cv_lambda = m_cv$lambda.min
m_optimal <- glmnet(x, y, alpha = 0, lambda = cv_lambda, standardize = TRUE)
y_hat_cv <- predict(m_optimal, x)
model_cv_rse = sqrt(sum((Y - y_hat_cv)^2) / (length(Y) - 2))
model_cv_rse
x <- model.matrix(duration ~ poly(pickup_x, pickup_y, dropoff_x, dropoff_y, weekday, hour_,  day_,distance,
month_, speed, degree = 1.1, raw = T), data = s6.250k.wlimits_ran)
y = s6.250k.wlimits_ran$duration
m_cv = cv.glmnet(x, y, alpha = 0, lambda = grid, standardize = TRUE, nfolds = 10)
cv_lambda = m_cv$lambda.min
m_optimal <- glmnet(x, y, alpha = 0, lambda = cv_lambda, standardize = TRUE)
y_hat_cv <- predict(m_optimal, x)
model_cv_rse = sqrt(sum((Y - y_hat_cv)^2) / (length(Y) - 2))
model_cv_rse
for (i in seq(1, 3.5, .1)){
print(paste('Training Model w/ Polynomial =>', i))
x <- model.matrix(duration ~ poly(pickup_x, speed, degree = i, raw = T), data = s6.250k.wlimits_ran)
y = s6.250k.wlimits_ran$duration
m_cv = cv.glmnet(x, y, alpha = 0, lambda = grid, standardize = TRUE, nfolds = 10)
cv_lambda = m_cv$lambda.min
m_optimal <- glmnet(x, y, alpha = 0, lambda = cv_lambda, standardize = TRUE)
y_hat_cv <- predict(m_optimal, x)
model_cv_rse = sqrt(sum((Y - y_hat_cv)^2) / (length(Y) - 2))
print(paste('Poly => ', i, ' TEST RSE => ', model_cv_rse))
}
for (i in seq(1, 3.5, .1)){
print(paste('Training Model w/ Polynomial =>', i))
x <- model.matrix(duration ~ poly(pickup_x, speed, degree = i, raw = T), data = s6.250k.wlimits_ran)
y = s6.250k.wlimits_ran$duration
m_cv = cv.glmnet(x, y, alpha = 0, lambda = grid, standardize = TRUE, nfolds = 10)
cv_lambda = m_cv$lambda.min
m_optimal <- glmnet(x, y, alpha = 0, lambda = cv_lambda, standardize = TRUE)
y_hat_cv <- predict(m_optimal, x)
model_cv_rse = sqrt(sum((Y - y_hat_cv)^2) / (length(Y) - 2))
print(paste('Poly => ', i, ' TEST RSE => ', round(model_cv_rse ,4)))
}
for (i in seq(1, 3.5, .1)){
print(paste('Training Model w/ Polynomial =>', i))
x <- model.matrix(duration ~ poly(pickup_x, speed, degree = i, raw = T), data = s6.250k.wlimits_ran)
y = s6.250k.wlimits_ran$duration
m_cv = cv.glmnet(x, y, alpha = 0, lambda = grid, standardize = TRUE, nfolds = 10)
cv_lambda = m_cv$lambda.min
m_optimal <- glmnet(x, y, alpha = 0, lambda = cv_lambda, standardize = TRUE)
y_hat_cv <- predict(m_optimal, x)
model_cv_rse = sqrt(sum((y - y_hat_cv)^2) / (length(Y) - 2))
print(paste('Poly => ', i, ' TEST RSE => ', round(model_cv_rse ,4)))
}
for (i in seq(1, 3.5, .1)){
print(paste('Training Model w/ Polynomial =>', i))
x <- model.matrix(duration ~ poly(pickup_x, pickup_y, dropoff_x, dropoff_y,
weekday, hour_,  day_,distance, month_, speed,
degree = i, raw = T), data = s6.250k.wlimits_ran)
y = s6.250k.wlimits_ran$duration
m_cv = cv.glmnet(x, y, alpha = 0, lambda = grid, standardize = TRUE, nfolds = 10)
cv_lambda = m_cv$lambda.min
m_optimal <- glmnet(x, y, alpha = 0, lambda = cv_lambda, standardize = TRUE)
y_hat_cv <- predict(m_optimal, x)
model_cv_rse = sqrt(sum((y - y_hat_cv)^2) / (length(Y) - 2))
print(paste('Poly => ', i, ' TEST RSE => ', round(model_cv_rse ,4)))
}
for (i in seq(1, 3.5, .1)){
print(paste('Training Model w/ Polynomial =>', i))
x <- model.matrix(duration ~ poly(pickup_x, pickup_y, dropoff_x, dropoff_y,
weekday, hour_,  day_,distance, month_, speed,
degree = i, raw = T), data = s4.50k.wlimits)
y = s4.50k.wlimits_ran$duration
m_cv = cv.glmnet(x, y, alpha = 0, lambda = grid, standardize = TRUE, nfolds = 10)
cv_lambda = m_cv$lambda.min
m_optimal <- glmnet(x, y, alpha = 0, lambda = cv_lambda, standardize = TRUE)
y_hat_cv <- predict(m_optimal, x)
model_cv_rse = sqrt(sum((y - y_hat_cv)^2) / (length(Y) - 2))
print(paste('Poly => ', i, ' TEST RSE => ', round(model_cv_rse ,4)))
}
for (i in seq(1, 3.5, .1)){
print(paste('Training Model w/ Polynomial =>', i))
x <- model.matrix(duration ~ poly(pickup_x, pickup_y, dropoff_x, dropoff_y,
weekday, hour_,  day_,distance, month_, speed,
degree = i, raw = T), data = s6.250k.wlimits_ran)
y = s6.250k.wlimits_ran$duration
m_cv = cv.glmnet(x, y, alpha = 0, lambda = grid, standardize = TRUE, nfolds = 10)
cv_lambda = m_cv$lambda.min
m_optimal <- glmnet(x, y, alpha = 0, lambda = cv_lambda, standardize = TRUE)
y_hat_cv <- predict(m_optimal, x)
model_cv_rse = sqrt(sum((y - y_hat_cv)^2) / (length(Y) - 2))
print(paste('Poly => ', i, ' TEST RSE => ', round(model_cv_rse ,4)))
}
for (i in seq(2, 3.5, .1)){
print(paste('Training Model w/ Polynomial =>', i))
x <- model.matrix(duration ~ poly(pickup_x, pickup_y, dropoff_x, dropoff_y,
weekday, hour_,  day_,distance, month_, speed,
degree = i, raw = T), data = s6.250k.wlimits_ran)
y = s6.250k.wlimits_ran$duration
m_cv = cv.glmnet(x, y, alpha = 0, lambda = grid, standardize = TRUE, nfolds = 10)
cv_lambda = m_cv$lambda.min
m_optimal <- glmnet(x, y, alpha = 0, lambda = cv_lambda, standardize = TRUE)
y_hat_cv <- predict(m_optimal, x)
model_cv_rse = sqrt(sum((y - y_hat_cv)^2) / (length(Y) - 2))
print(paste('Poly => ', i, ' TEST RSE => ', round(model_cv_rse ,4)))
}
